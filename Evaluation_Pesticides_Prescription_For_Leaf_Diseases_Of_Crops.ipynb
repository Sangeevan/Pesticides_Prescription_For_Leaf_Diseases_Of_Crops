{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining necessary file paths\n",
    "\n",
    "model_path = '../Model/Pesticides_Prescription_Model.sav'\n",
    "labels_path = '../Model/Class_Labels_Pesticides_Prescription_Model.csv'\n",
    "test_data_dir = '../Dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=15, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restoring the model\n",
    "\n",
    "def initialize_alexnet(model_name, num_classes, feature_extract=True, use_pretrained=True):\n",
    "\n",
    "  model_ft = None\n",
    "  input_size = 0\n",
    "\n",
    "  if model_name == \"alexnet\":\n",
    "    model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "\n",
    "    if feature_extract:\n",
    "        for param in model_ft.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    num_ftrs = model_ft.classifier[6].in_features\n",
    "    model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "\n",
    "    input_size = 224\n",
    "  else:\n",
    "        print(\"Model name is invalid!\")\n",
    "        exit()\n",
    "\n",
    "  return model_ft,input_size\n",
    "\n",
    "model, _ = initialize_alexnet(\"alexnet\", num_classes=15)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the labels file\n",
    "\n",
    "labels_df = pd.read_csv(labels_path, header=None, names=['idx','Label'])\n",
    "labels_dict = dict(zip(labels_df.idx, labels_df.Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the output\n",
    "\n",
    "def predict(img):\n",
    "    transform = transforms.Compose([            \n",
    "    transforms.Resize(256),                    \n",
    "    transforms.CenterCrop(224),                \n",
    "    transforms.ToTensor(),                     \n",
    "    transforms.Normalize(                      \n",
    "    mean=[0.485, 0.456, 0.406],                \n",
    "    std=[0.229, 0.224, 0.225]                  \n",
    "    )])\n",
    "\n",
    "    img_t = transform(img)\n",
    "    batch_t = torch.unsqueeze(img_t, 0)  \n",
    "    out = model(batch_t)\n",
    "    _, index = torch.max(out, 1)\n",
    "    \n",
    "    return index\n",
    "\n",
    "def get_label(labels_dict, idx=-1):\n",
    "    if (idx>=0 and idx<=14):\n",
    "        label = labels_dict[idx]\n",
    "    else:\n",
    "        label = ''\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting actual and predicted values for the test dataset\n",
    "\n",
    "actual_label_list = []\n",
    "predicted_lebel_list = []\n",
    "list_test_data_dir = listdir(test_data_dir)\n",
    "for leaf_image_dir in list_test_data_dir :\n",
    "        image_list = listdir(f\"{test_data_dir}/{leaf_image_dir}\")\n",
    "        for image in image_list:\n",
    "          image_file = f\"{test_data_dir}/{leaf_image_dir}/{image}\"\n",
    "          if image_file.endswith(\".jpg\") == True or image_file.endswith(\".JPG\") == True:\n",
    "            actual_label_list.append(leaf_image_dir)\n",
    "            img = Image.open(image_file)\n",
    "            pred_idx = predict(img)\n",
    "            predicted_lebel = get_label(labels_dict, pred_idx.numpy()[0])\n",
    "            predicted_lebel_list.append(predicted_lebel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 96,   4,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  1, 148,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  1,   0,  98,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,  15,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   9,   7,  64,   3,   0,   0,   8,   1,   0,   3,   1,\n",
       "          4,   0],\n",
       "       [  0,   0,   0,   0,   0, 207,   0,   0,   0,   0,   0,   2,   4,\n",
       "          0,   1],\n",
       "       [  3,   0,   0,   0,   1,  10,  48,   1,   9,   4,   0,   7,  10,\n",
       "          3,   4],\n",
       "       [  0,   1,   0,   0,   0,   0,   0, 158,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   4,   3,   2,   4, 159,   5,   0,   3,   5,\n",
       "          2,   5],\n",
       "       [  0,   0,   0,   0,   0,   1,   0,   1,   1,  85,   0,   2,   1,\n",
       "          1,   4],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  37,   0,   0,\n",
       "          1,   0],\n",
       "       [  0,   3,   1,   0,   0,   1,   0,   1,   1,   1,   3, 164,   3,\n",
       "          0,   0],\n",
       "       [  0,   1,   0,   0,   0,   1,   0,  17,   0,   0,   0,   1, 110,\n",
       "         11,   0],\n",
       "       [  2,   0,   0,   0,   0,   0,   0,   8,   0,   0,   0,   0,  17,\n",
       "        136,   6],\n",
       "       [  1,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,   0,\n",
       "          0, 163]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "cm = confusion_matrix(actual_label_list, predicted_lebel_list)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8800834202294057"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "\n",
    "accuracy = (cm.trace()/cm.sum())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92307692, 0.94267516, 0.89908257, 0.68181818, 0.91428571,\n",
       "       0.9159292 , 0.96      , 0.83157895, 0.89325843, 0.87628866,\n",
       "       0.925     , 0.89617486, 0.72847682, 0.86075949, 0.89071038])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision\n",
    "\n",
    "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8759410230437837"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Macro average precision\n",
    "\n",
    "macro_average_precision = np.mean(precision)\n",
    "macro_average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95049505, 0.99328859, 0.98      , 0.9375    , 0.64      ,\n",
       "       0.96728972, 0.48      , 0.99371069, 0.828125  , 0.88541667,\n",
       "       0.97368421, 0.92134831, 0.78014184, 0.80473373, 0.98787879])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recall\n",
    "\n",
    "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8749075068679892"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Macro average recall\n",
    "\n",
    "macro_average_recall = np.mean(recall)\n",
    "macro_average_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93658537, 0.96732026, 0.93779904, 0.78947368, 0.75294118,\n",
       "       0.94090909, 0.64      , 0.90544413, 0.85945946, 0.88082902,\n",
       "       0.94871795, 0.90858726, 0.75342466, 0.83180428, 0.93678161])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F1 score\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8660051318288576"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Macro average F1 score\n",
    "\n",
    "macro_average_F1 = np.mean(F1)\n",
    "macro_average_F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "    Bell_pepper__Bacterial_spot       0.92      0.95      0.94       101\n",
      "           Bell_pepper__Healthy       0.94      0.99      0.97       149\n",
      "           Potato__Early_blight       0.90      0.98      0.94       100\n",
      "                Potato__Healthy       0.68      0.94      0.79        16\n",
      "            Potato__Late_blight       0.91      0.64      0.75       100\n",
      "         Tomato__Bacterial_spot       0.92      0.97      0.94       214\n",
      "           Tomato__Early_blight       0.96      0.48      0.64       100\n",
      "                Tomato__Healthy       0.83      0.99      0.91       159\n",
      "            Tomato__Late_blight       0.89      0.83      0.86       192\n",
      "              Tomato__Leaf_mold       0.88      0.89      0.88        96\n",
      "           Tomato__Mosaic_virus       0.93      0.97      0.95        38\n",
      "     Tomato__Septoria_leaf_spot       0.90      0.92      0.91       178\n",
      "            Tomato__Target_spot       0.73      0.78      0.75       141\n",
      "Tomato__Two_spotted_spider_mite       0.86      0.80      0.83       169\n",
      " Tomato__Yellow_leaf_curl_virus       0.89      0.99      0.94       165\n",
      "\n",
      "                       accuracy                           0.88      1918\n",
      "                      macro avg       0.88      0.87      0.87      1918\n",
      "                   weighted avg       0.88      0.88      0.88      1918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "\n",
    "print(classification_report(actual_label_list, predicted_lebel_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
