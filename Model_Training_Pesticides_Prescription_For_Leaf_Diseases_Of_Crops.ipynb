{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the needed parameters\n",
    "\n",
    "dataset_dir = '../Dataset'\n",
    "model_name = 'alexnet'\n",
    "num_classes = 15\n",
    "batch_size = 128\n",
    "num_epochs = 500\n",
    "feature_extract = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(dataset_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Detecting GPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce 840M'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the GPU name\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=15, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Reshaping and initializing the model\n",
    "\n",
    "def initialize_alexnet(model_name, num_classes, feature_extract=True, use_pretrained=True):\n",
    "\n",
    "  model_ft = None\n",
    "  input_size = 0\n",
    "\n",
    "  if model_name == \"alexnet\":\n",
    "    model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "\n",
    "    if feature_extract:\n",
    "        for param in model_ft.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    num_ftrs = model_ft.classifier[6].in_features\n",
    "    model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "\n",
    "    input_size = 224\n",
    "  else:\n",
    "        print(\"Model name is invalid!\")\n",
    "        exit()\n",
    "\n",
    "  return model_ft,input_size\n",
    "\n",
    "model_ft, input_size = initialize_alexnet(model_name, num_classes, feature_extract)\n",
    "\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to learn\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "# Creating the optimizer\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Parameters to learn\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500\n",
      "------------------------------\n",
      "train Loss: 1.2188 Acc: 0.6366\n",
      "val Loss: 0.8985 Acc: 0.7633\n",
      "\n",
      "Epoch: 2/500\n",
      "------------------------------\n",
      "train Loss: 0.9753 Acc: 0.7075\n",
      "val Loss: 0.8462 Acc: 0.7985\n",
      "\n",
      "Epoch: 3/500\n",
      "------------------------------\n",
      "train Loss: 0.9371 Acc: 0.7241\n",
      "val Loss: 0.7071 Acc: 0.8106\n",
      "\n",
      "Epoch: 4/500\n",
      "------------------------------\n",
      "train Loss: 0.9057 Acc: 0.7349\n",
      "val Loss: 0.8687 Acc: 0.7964\n",
      "\n",
      "Epoch: 5/500\n",
      "------------------------------\n",
      "train Loss: 0.9502 Acc: 0.7311\n",
      "val Loss: 0.4859 Acc: 0.8564\n",
      "\n",
      "Epoch: 6/500\n",
      "------------------------------\n",
      "train Loss: 0.8995 Acc: 0.7415\n",
      "val Loss: 0.5959 Acc: 0.8327\n",
      "\n",
      "Epoch: 7/500\n",
      "------------------------------\n",
      "train Loss: 0.8968 Acc: 0.7414\n",
      "val Loss: 0.8496 Acc: 0.7896\n",
      "\n",
      "Epoch: 8/500\n",
      "------------------------------\n",
      "train Loss: 0.8986 Acc: 0.7467\n",
      "val Loss: 1.2611 Acc: 0.7454\n",
      "\n",
      "Epoch: 9/500\n",
      "------------------------------\n",
      "train Loss: 0.8722 Acc: 0.7523\n",
      "val Loss: 0.6390 Acc: 0.8385\n",
      "\n",
      "Epoch: 10/500\n",
      "------------------------------\n",
      "train Loss: 0.8437 Acc: 0.7601\n",
      "val Loss: 0.6798 Acc: 0.8101\n",
      "\n",
      "Epoch: 11/500\n",
      "------------------------------\n",
      "train Loss: 0.8854 Acc: 0.7523\n",
      "val Loss: 0.6937 Acc: 0.8127\n",
      "\n",
      "Epoch: 12/500\n",
      "------------------------------\n",
      "train Loss: 0.8592 Acc: 0.7560\n",
      "val Loss: 0.9380 Acc: 0.7933\n",
      "\n",
      "Epoch: 13/500\n",
      "------------------------------\n",
      "train Loss: 0.9080 Acc: 0.7561\n",
      "val Loss: 0.5742 Acc: 0.8364\n",
      "\n",
      "Epoch: 14/500\n",
      "------------------------------\n",
      "train Loss: 0.8665 Acc: 0.7604\n",
      "val Loss: 0.6138 Acc: 0.8254\n",
      "\n",
      "Epoch: 15/500\n",
      "------------------------------\n",
      "train Loss: 0.8487 Acc: 0.7601\n",
      "val Loss: 0.5649 Acc: 0.8417\n",
      "\n",
      "Epoch: 16/500\n",
      "------------------------------\n",
      "train Loss: 0.8449 Acc: 0.7587\n",
      "val Loss: 0.9704 Acc: 0.7906\n",
      "\n",
      "Epoch: 17/500\n",
      "------------------------------\n",
      "train Loss: 0.9150 Acc: 0.7543\n",
      "val Loss: 0.8100 Acc: 0.7980\n",
      "\n",
      "Epoch: 18/500\n",
      "------------------------------\n",
      "train Loss: 0.9290 Acc: 0.7561\n",
      "val Loss: 0.7234 Acc: 0.8317\n",
      "\n",
      "Epoch: 19/500\n",
      "------------------------------\n",
      "train Loss: 0.8432 Acc: 0.7640\n",
      "val Loss: 0.6403 Acc: 0.8364\n",
      "\n",
      "Epoch: 20/500\n",
      "------------------------------\n",
      "train Loss: 0.8450 Acc: 0.7668\n",
      "val Loss: 0.5472 Acc: 0.8496\n",
      "\n",
      "Epoch: 21/500\n",
      "------------------------------\n",
      "train Loss: 0.8534 Acc: 0.7626\n",
      "val Loss: 0.5212 Acc: 0.8617\n",
      "\n",
      "Epoch: 22/500\n",
      "------------------------------\n",
      "train Loss: 0.8253 Acc: 0.7685\n",
      "val Loss: 0.6264 Acc: 0.8311\n",
      "\n",
      "Epoch: 23/500\n",
      "------------------------------\n",
      "train Loss: 0.8234 Acc: 0.7702\n",
      "val Loss: 0.6518 Acc: 0.8317\n",
      "\n",
      "Epoch: 24/500\n",
      "------------------------------\n",
      "train Loss: 0.8822 Acc: 0.7624\n",
      "val Loss: 0.8454 Acc: 0.8059\n",
      "\n",
      "Epoch: 25/500\n",
      "------------------------------\n",
      "train Loss: 0.8885 Acc: 0.7594\n",
      "val Loss: 0.5777 Acc: 0.8432\n",
      "\n",
      "Epoch: 26/500\n",
      "------------------------------\n",
      "train Loss: 0.8786 Acc: 0.7644\n",
      "val Loss: 0.9099 Acc: 0.8090\n",
      "\n",
      "Epoch: 27/500\n",
      "------------------------------\n",
      "train Loss: 0.8828 Acc: 0.7635\n",
      "val Loss: 0.7594 Acc: 0.8112\n",
      "\n",
      "Epoch: 28/500\n",
      "------------------------------\n",
      "train Loss: 0.8114 Acc: 0.7708\n",
      "val Loss: 1.0214 Acc: 0.7912\n",
      "\n",
      "Epoch: 29/500\n",
      "------------------------------\n",
      "train Loss: 0.8176 Acc: 0.7736\n",
      "val Loss: 0.7867 Acc: 0.8233\n",
      "\n",
      "Epoch: 30/500\n",
      "------------------------------\n",
      "train Loss: 0.8091 Acc: 0.7743\n",
      "val Loss: 0.6719 Acc: 0.8353\n",
      "\n",
      "Epoch: 31/500\n",
      "------------------------------\n",
      "train Loss: 0.8331 Acc: 0.7719\n",
      "val Loss: 0.6397 Acc: 0.8417\n",
      "\n",
      "Epoch: 32/500\n",
      "------------------------------\n",
      "train Loss: 0.8562 Acc: 0.7678\n",
      "val Loss: 0.5776 Acc: 0.8490\n",
      "\n",
      "Epoch: 33/500\n",
      "------------------------------\n",
      "train Loss: 0.8366 Acc: 0.7703\n",
      "val Loss: 0.5245 Acc: 0.8622\n",
      "\n",
      "Epoch: 34/500\n",
      "------------------------------\n",
      "train Loss: 0.8527 Acc: 0.7698\n",
      "val Loss: 0.6069 Acc: 0.8569\n",
      "\n",
      "Epoch: 35/500\n",
      "------------------------------\n",
      "train Loss: 0.8069 Acc: 0.7731\n",
      "val Loss: 1.0266 Acc: 0.7891\n",
      "\n",
      "Epoch: 36/500\n",
      "------------------------------\n",
      "train Loss: 0.9725 Acc: 0.7552\n",
      "val Loss: 0.4879 Acc: 0.8732\n",
      "\n",
      "Epoch: 37/500\n",
      "------------------------------\n",
      "train Loss: 0.8715 Acc: 0.7685\n",
      "val Loss: 0.4663 Acc: 0.8743\n",
      "\n",
      "Epoch: 38/500\n",
      "------------------------------\n",
      "train Loss: 0.8371 Acc: 0.7750\n",
      "val Loss: 0.6033 Acc: 0.8474\n",
      "\n",
      "Epoch: 39/500\n",
      "------------------------------\n",
      "train Loss: 0.9274 Acc: 0.7563\n",
      "val Loss: 0.9787 Acc: 0.8080\n",
      "\n",
      "Epoch: 40/500\n",
      "------------------------------\n",
      "train Loss: 0.8587 Acc: 0.7704\n",
      "val Loss: 0.5248 Acc: 0.8716\n",
      "\n",
      "Epoch: 41/500\n",
      "------------------------------\n",
      "train Loss: 0.8290 Acc: 0.7741\n",
      "val Loss: 0.5488 Acc: 0.8653\n",
      "\n",
      "Epoch: 42/500\n",
      "------------------------------\n",
      "train Loss: 0.8528 Acc: 0.7677\n",
      "val Loss: 0.8990 Acc: 0.8196\n",
      "\n",
      "Epoch: 43/500\n",
      "------------------------------\n",
      "train Loss: 0.8390 Acc: 0.7727\n",
      "val Loss: 0.6129 Acc: 0.8474\n",
      "\n",
      "Epoch: 44/500\n",
      "------------------------------\n",
      "train Loss: 0.8435 Acc: 0.7722\n",
      "val Loss: 0.6910 Acc: 0.8453\n",
      "\n",
      "Epoch: 45/500\n",
      "------------------------------\n",
      "train Loss: 0.8534 Acc: 0.7721\n",
      "val Loss: 0.7131 Acc: 0.8427\n",
      "\n",
      "Epoch: 46/500\n",
      "------------------------------\n",
      "train Loss: 0.8595 Acc: 0.7686\n",
      "val Loss: 0.6472 Acc: 0.8343\n",
      "\n",
      "Epoch: 47/500\n",
      "------------------------------\n",
      "train Loss: 0.8146 Acc: 0.7799\n",
      "val Loss: 0.6545 Acc: 0.8438\n",
      "\n",
      "Epoch: 48/500\n",
      "------------------------------\n",
      "train Loss: 0.8433 Acc: 0.7713\n",
      "val Loss: 0.7045 Acc: 0.8375\n",
      "\n",
      "Epoch: 49/500\n",
      "------------------------------\n",
      "train Loss: 0.8519 Acc: 0.7726\n",
      "val Loss: 0.8718 Acc: 0.8164\n",
      "\n",
      "Epoch: 50/500\n",
      "------------------------------\n",
      "train Loss: 0.8947 Acc: 0.7649\n",
      "val Loss: 0.7124 Acc: 0.8290\n",
      "\n",
      "Epoch: 51/500\n",
      "------------------------------\n",
      "train Loss: 0.8949 Acc: 0.7700\n",
      "val Loss: 0.7048 Acc: 0.8301\n",
      "\n",
      "Epoch: 52/500\n",
      "------------------------------\n",
      "train Loss: 0.9822 Acc: 0.7605\n",
      "val Loss: 0.7682 Acc: 0.8217\n",
      "\n",
      "Epoch: 53/500\n",
      "------------------------------\n",
      "train Loss: 0.8425 Acc: 0.7744\n",
      "val Loss: 0.5827 Acc: 0.8595\n",
      "\n",
      "Epoch: 54/500\n",
      "------------------------------\n",
      "train Loss: 0.8718 Acc: 0.7715\n",
      "val Loss: 0.8269 Acc: 0.8322\n",
      "\n",
      "Epoch: 55/500\n",
      "------------------------------\n",
      "train Loss: 0.8971 Acc: 0.7684\n",
      "val Loss: 0.6116 Acc: 0.8548\n",
      "\n",
      "Epoch: 56/500\n",
      "------------------------------\n",
      "train Loss: 0.8424 Acc: 0.7753\n",
      "val Loss: 0.7408 Acc: 0.8269\n",
      "\n",
      "Epoch: 57/500\n",
      "------------------------------\n",
      "train Loss: 0.8331 Acc: 0.7716\n",
      "val Loss: 0.6006 Acc: 0.8527\n",
      "\n",
      "Epoch: 58/500\n",
      "------------------------------\n",
      "train Loss: 0.8662 Acc: 0.7691\n",
      "val Loss: 0.7263 Acc: 0.8301\n",
      "\n",
      "Epoch: 59/500\n",
      "------------------------------\n",
      "train Loss: 0.8348 Acc: 0.7782\n",
      "val Loss: 0.5970 Acc: 0.8522\n",
      "\n",
      "Epoch: 60/500\n",
      "------------------------------\n",
      "train Loss: 0.8424 Acc: 0.7695\n",
      "val Loss: 0.6362 Acc: 0.8511\n",
      "\n",
      "Epoch: 61/500\n",
      "------------------------------\n",
      "train Loss: 0.8545 Acc: 0.7729\n",
      "val Loss: 0.7066 Acc: 0.8301\n",
      "\n",
      "Epoch: 62/500\n",
      "------------------------------\n",
      "train Loss: 0.8103 Acc: 0.7812\n",
      "val Loss: 0.6139 Acc: 0.8553\n",
      "\n",
      "Epoch: 63/500\n",
      "------------------------------\n",
      "train Loss: 0.8475 Acc: 0.7739\n",
      "val Loss: 0.6142 Acc: 0.8532\n",
      "\n",
      "Epoch: 64/500\n",
      "------------------------------\n",
      "train Loss: 0.9114 Acc: 0.7692\n",
      "val Loss: 0.6255 Acc: 0.8543\n",
      "\n",
      "Epoch: 65/500\n",
      "------------------------------\n",
      "train Loss: 0.8386 Acc: 0.7747\n",
      "val Loss: 0.8939 Acc: 0.8043\n",
      "\n",
      "Epoch: 66/500\n",
      "------------------------------\n",
      "train Loss: 0.8696 Acc: 0.7716\n",
      "val Loss: 0.6434 Acc: 0.8453\n",
      "\n",
      "Epoch: 67/500\n",
      "------------------------------\n",
      "train Loss: 0.8395 Acc: 0.7723\n",
      "val Loss: 0.6868 Acc: 0.8401\n",
      "\n",
      "Epoch: 68/500\n",
      "------------------------------\n",
      "train Loss: 0.8213 Acc: 0.7782\n",
      "val Loss: 0.5249 Acc: 0.8622\n",
      "\n",
      "Epoch: 69/500\n",
      "------------------------------\n",
      "train Loss: 0.8096 Acc: 0.7810\n",
      "val Loss: 0.6752 Acc: 0.8464\n",
      "\n",
      "Epoch: 70/500\n",
      "------------------------------\n",
      "train Loss: 0.8364 Acc: 0.7725\n",
      "val Loss: 0.7134 Acc: 0.8285\n",
      "\n",
      "Epoch: 71/500\n",
      "------------------------------\n",
      "train Loss: 0.8144 Acc: 0.7788\n",
      "val Loss: 0.5164 Acc: 0.8659\n",
      "\n",
      "Epoch: 72/500\n",
      "------------------------------\n",
      "train Loss: 0.7880 Acc: 0.7841\n",
      "val Loss: 0.6029 Acc: 0.8548\n",
      "\n",
      "Epoch: 73/500\n",
      "------------------------------\n",
      "train Loss: 0.8238 Acc: 0.7780\n",
      "val Loss: 0.7326 Acc: 0.8364\n",
      "\n",
      "Epoch: 74/500\n",
      "------------------------------\n",
      "train Loss: 0.8067 Acc: 0.7749\n",
      "val Loss: 0.5670 Acc: 0.8543\n",
      "\n",
      "Epoch: 75/500\n",
      "------------------------------\n",
      "train Loss: 0.8627 Acc: 0.7743\n",
      "val Loss: 0.8729 Acc: 0.8180\n",
      "\n",
      "Epoch: 76/500\n",
      "------------------------------\n",
      "train Loss: 0.8251 Acc: 0.7805\n",
      "val Loss: 0.5422 Acc: 0.8559\n",
      "\n",
      "Epoch: 77/500\n",
      "------------------------------\n",
      "train Loss: 0.8584 Acc: 0.7730\n",
      "val Loss: 0.8910 Acc: 0.8106\n",
      "\n",
      "Epoch: 78/500\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.8309 Acc: 0.7746\n",
      "val Loss: 0.5052 Acc: 0.8643\n",
      "\n",
      "Epoch: 79/500\n",
      "------------------------------\n",
      "train Loss: 0.8013 Acc: 0.7791\n",
      "val Loss: 0.7477 Acc: 0.8127\n",
      "\n",
      "Epoch: 80/500\n",
      "------------------------------\n",
      "train Loss: 0.8574 Acc: 0.7736\n",
      "val Loss: 0.5127 Acc: 0.8643\n",
      "\n",
      "Epoch: 81/500\n",
      "------------------------------\n",
      "train Loss: 0.8234 Acc: 0.7788\n",
      "val Loss: 0.5026 Acc: 0.8680\n",
      "\n",
      "Epoch: 82/500\n",
      "------------------------------\n",
      "train Loss: 0.8319 Acc: 0.7727\n",
      "val Loss: 0.8693 Acc: 0.8217\n",
      "\n",
      "Epoch: 83/500\n",
      "------------------------------\n",
      "train Loss: 0.8143 Acc: 0.7820\n",
      "val Loss: 0.9825 Acc: 0.8054\n",
      "\n",
      "Epoch: 84/500\n",
      "------------------------------\n",
      "train Loss: 0.8617 Acc: 0.7738\n",
      "val Loss: 0.4449 Acc: 0.8843\n",
      "\n",
      "Epoch: 85/500\n",
      "------------------------------\n",
      "train Loss: 0.7957 Acc: 0.7807\n",
      "val Loss: 0.8059 Acc: 0.8101\n",
      "\n",
      "Epoch: 86/500\n",
      "------------------------------\n",
      "train Loss: 0.8500 Acc: 0.7739\n",
      "val Loss: 0.4947 Acc: 0.8690\n",
      "\n",
      "Epoch: 87/500\n",
      "------------------------------\n",
      "train Loss: 0.8267 Acc: 0.7754\n",
      "val Loss: 0.6706 Acc: 0.8364\n",
      "\n",
      "Epoch: 88/500\n",
      "------------------------------\n",
      "train Loss: 0.8515 Acc: 0.7782\n",
      "val Loss: 0.6838 Acc: 0.8385\n",
      "\n",
      "Epoch: 89/500\n",
      "------------------------------\n",
      "train Loss: 0.8209 Acc: 0.7824\n",
      "val Loss: 0.5184 Acc: 0.8711\n",
      "\n",
      "Epoch: 90/500\n",
      "------------------------------\n",
      "train Loss: 0.8334 Acc: 0.7790\n",
      "val Loss: 0.5673 Acc: 0.8580\n",
      "\n",
      "Epoch: 91/500\n",
      "------------------------------\n",
      "train Loss: 0.8190 Acc: 0.7799\n",
      "val Loss: 0.6093 Acc: 0.8522\n",
      "\n",
      "Epoch: 92/500\n",
      "------------------------------\n",
      "train Loss: 0.8100 Acc: 0.7824\n",
      "val Loss: 0.5826 Acc: 0.8511\n",
      "\n",
      "Epoch: 93/500\n",
      "------------------------------\n",
      "train Loss: 0.7981 Acc: 0.7811\n",
      "val Loss: 0.8220 Acc: 0.8254\n",
      "\n",
      "Epoch: 94/500\n",
      "------------------------------\n",
      "train Loss: 0.8425 Acc: 0.7727\n",
      "val Loss: 0.5788 Acc: 0.8559\n",
      "\n",
      "Epoch: 95/500\n",
      "------------------------------\n",
      "train Loss: 0.8036 Acc: 0.7788\n",
      "val Loss: 0.4877 Acc: 0.8738\n",
      "\n",
      "Epoch: 96/500\n",
      "------------------------------\n",
      "train Loss: 0.7820 Acc: 0.7827\n",
      "val Loss: 0.4967 Acc: 0.8759\n",
      "\n",
      "Epoch: 97/500\n",
      "------------------------------\n",
      "train Loss: 0.8373 Acc: 0.7736\n",
      "val Loss: 0.5953 Acc: 0.8527\n",
      "\n",
      "Epoch: 98/500\n",
      "------------------------------\n",
      "train Loss: 0.8442 Acc: 0.7752\n",
      "val Loss: 0.9022 Acc: 0.8233\n",
      "\n",
      "Epoch: 99/500\n",
      "------------------------------\n",
      "train Loss: 0.8415 Acc: 0.7761\n",
      "val Loss: 0.5697 Acc: 0.8448\n",
      "\n",
      "Epoch: 100/500\n",
      "------------------------------\n",
      "train Loss: 0.8120 Acc: 0.7778\n",
      "val Loss: 0.7281 Acc: 0.8348\n",
      "\n",
      "Epoch: 101/500\n",
      "------------------------------\n",
      "train Loss: 0.8762 Acc: 0.7727\n",
      "val Loss: 0.5354 Acc: 0.8590\n",
      "\n",
      "Epoch: 102/500\n",
      "------------------------------\n",
      "train Loss: 0.8224 Acc: 0.7785\n",
      "val Loss: 0.7363 Acc: 0.8180\n",
      "\n",
      "Epoch: 103/500\n",
      "------------------------------\n",
      "train Loss: 0.8124 Acc: 0.7815\n",
      "val Loss: 0.6482 Acc: 0.8332\n",
      "\n",
      "Epoch: 104/500\n",
      "------------------------------\n",
      "train Loss: 0.8028 Acc: 0.7811\n",
      "val Loss: 0.6808 Acc: 0.8338\n",
      "\n",
      "Epoch: 105/500\n",
      "------------------------------\n",
      "train Loss: 0.7970 Acc: 0.7786\n",
      "val Loss: 0.6497 Acc: 0.8348\n",
      "\n",
      "Epoch: 106/500\n",
      "------------------------------\n",
      "train Loss: 0.7988 Acc: 0.7863\n",
      "val Loss: 0.6243 Acc: 0.8522\n",
      "\n",
      "Epoch: 107/500\n",
      "------------------------------\n",
      "train Loss: 0.8357 Acc: 0.7752\n",
      "val Loss: 0.5956 Acc: 0.8527\n",
      "\n",
      "Epoch: 108/500\n",
      "------------------------------\n",
      "train Loss: 0.8327 Acc: 0.7794\n",
      "val Loss: 0.8253 Acc: 0.8080\n",
      "\n",
      "Epoch: 109/500\n",
      "------------------------------\n",
      "train Loss: 0.8672 Acc: 0.7706\n",
      "val Loss: 0.5658 Acc: 0.8722\n",
      "\n",
      "Epoch: 110/500\n",
      "------------------------------\n",
      "train Loss: 0.8265 Acc: 0.7803\n",
      "val Loss: 0.5817 Acc: 0.8617\n",
      "\n",
      "Epoch: 111/500\n",
      "------------------------------\n",
      "train Loss: 0.8323 Acc: 0.7742\n",
      "val Loss: 0.5517 Acc: 0.8601\n",
      "\n",
      "Epoch: 112/500\n",
      "------------------------------\n",
      "train Loss: 0.8717 Acc: 0.7773\n",
      "val Loss: 0.5307 Acc: 0.8538\n",
      "\n",
      "Epoch: 113/500\n",
      "------------------------------\n",
      "train Loss: 0.8419 Acc: 0.7766\n",
      "val Loss: 0.5099 Acc: 0.8659\n",
      "\n",
      "Epoch: 114/500\n",
      "------------------------------\n",
      "train Loss: 0.8413 Acc: 0.7757\n",
      "val Loss: 0.6126 Acc: 0.8496\n",
      "\n",
      "Epoch: 115/500\n",
      "------------------------------\n",
      "train Loss: 0.7940 Acc: 0.7805\n",
      "val Loss: 0.5860 Acc: 0.8522\n",
      "\n",
      "Epoch: 116/500\n",
      "------------------------------\n",
      "train Loss: 0.8552 Acc: 0.7765\n",
      "val Loss: 0.6282 Acc: 0.8464\n",
      "\n",
      "Epoch: 117/500\n",
      "------------------------------\n",
      "train Loss: 0.8579 Acc: 0.7757\n",
      "val Loss: 0.6263 Acc: 0.8574\n",
      "\n",
      "Epoch: 118/500\n",
      "------------------------------\n",
      "train Loss: 0.8450 Acc: 0.7761\n",
      "val Loss: 0.6629 Acc: 0.8422\n",
      "\n",
      "Epoch: 119/500\n",
      "------------------------------\n",
      "train Loss: 0.8241 Acc: 0.7812\n",
      "val Loss: 0.5857 Acc: 0.8490\n",
      "\n",
      "Epoch: 120/500\n",
      "------------------------------\n",
      "train Loss: 0.8497 Acc: 0.7776\n",
      "val Loss: 0.6178 Acc: 0.8506\n",
      "\n",
      "Epoch: 121/500\n",
      "------------------------------\n",
      "train Loss: 0.7961 Acc: 0.7851\n",
      "val Loss: 0.5337 Acc: 0.8706\n",
      "\n",
      "Epoch: 122/500\n",
      "------------------------------\n",
      "train Loss: 0.7880 Acc: 0.7875\n",
      "val Loss: 0.6261 Acc: 0.8480\n",
      "\n",
      "Epoch: 123/500\n",
      "------------------------------\n",
      "train Loss: 0.7758 Acc: 0.7881\n",
      "val Loss: 1.0110 Acc: 0.7864\n",
      "\n",
      "Epoch: 124/500\n",
      "------------------------------\n",
      "train Loss: 0.8491 Acc: 0.7777\n",
      "val Loss: 0.4950 Acc: 0.8722\n",
      "\n",
      "Epoch: 125/500\n",
      "------------------------------\n",
      "train Loss: 0.8092 Acc: 0.7819\n",
      "val Loss: 0.6530 Acc: 0.8353\n",
      "\n",
      "Epoch: 126/500\n",
      "------------------------------\n",
      "train Loss: 0.8112 Acc: 0.7832\n",
      "val Loss: 0.6351 Acc: 0.8490\n",
      "\n",
      "Epoch: 127/500\n",
      "------------------------------\n",
      "train Loss: 0.8476 Acc: 0.7758\n",
      "val Loss: 0.7364 Acc: 0.8332\n",
      "\n",
      "Epoch: 128/500\n",
      "------------------------------\n",
      "train Loss: 0.8095 Acc: 0.7827\n",
      "val Loss: 0.4913 Acc: 0.8606\n",
      "\n",
      "Epoch: 129/500\n",
      "------------------------------\n",
      "train Loss: 0.8189 Acc: 0.7825\n",
      "val Loss: 0.4921 Acc: 0.8716\n",
      "\n",
      "Epoch: 130/500\n",
      "------------------------------\n",
      "train Loss: 0.8155 Acc: 0.7799\n",
      "val Loss: 0.5976 Acc: 0.8601\n",
      "\n",
      "Epoch: 131/500\n",
      "------------------------------\n",
      "train Loss: 0.7711 Acc: 0.7895\n",
      "val Loss: 0.7844 Acc: 0.8248\n",
      "\n",
      "Epoch: 132/500\n",
      "------------------------------\n",
      "train Loss: 0.7820 Acc: 0.7845\n",
      "val Loss: 0.6577 Acc: 0.8390\n",
      "\n",
      "Epoch: 133/500\n",
      "------------------------------\n",
      "train Loss: 0.8065 Acc: 0.7797\n",
      "val Loss: 0.5725 Acc: 0.8632\n",
      "\n",
      "Epoch: 134/500\n",
      "------------------------------\n",
      "train Loss: 0.8233 Acc: 0.7778\n",
      "val Loss: 0.7495 Acc: 0.8196\n",
      "\n",
      "Epoch: 135/500\n",
      "------------------------------\n",
      "train Loss: 0.7784 Acc: 0.7878\n",
      "val Loss: 0.8706 Acc: 0.8080\n",
      "\n",
      "Epoch: 136/500\n",
      "------------------------------\n",
      "train Loss: 0.8076 Acc: 0.7796\n",
      "val Loss: 0.7090 Acc: 0.8122\n",
      "\n",
      "Epoch: 137/500\n",
      "------------------------------\n",
      "train Loss: 0.8042 Acc: 0.7796\n",
      "val Loss: 0.7041 Acc: 0.8285\n",
      "\n",
      "Epoch: 138/500\n",
      "------------------------------\n",
      "train Loss: 0.8323 Acc: 0.7805\n",
      "val Loss: 0.4961 Acc: 0.8669\n",
      "\n",
      "Epoch: 139/500\n",
      "------------------------------\n",
      "train Loss: 0.8159 Acc: 0.7780\n",
      "val Loss: 0.7416 Acc: 0.8164\n",
      "\n",
      "Epoch: 140/500\n",
      "------------------------------\n",
      "train Loss: 0.7711 Acc: 0.7896\n",
      "val Loss: 0.5576 Acc: 0.8532\n",
      "\n",
      "Epoch: 141/500\n",
      "------------------------------\n",
      "train Loss: 0.7965 Acc: 0.7801\n",
      "val Loss: 0.7775 Acc: 0.8348\n",
      "\n",
      "Epoch: 142/500\n",
      "------------------------------\n",
      "train Loss: 0.8431 Acc: 0.7803\n",
      "val Loss: 0.5029 Acc: 0.8643\n",
      "\n",
      "Epoch: 143/500\n",
      "------------------------------\n",
      "train Loss: 0.7948 Acc: 0.7877\n",
      "val Loss: 0.6913 Acc: 0.8375\n",
      "\n",
      "Epoch: 144/500\n",
      "------------------------------\n",
      "train Loss: 0.8405 Acc: 0.7755\n",
      "val Loss: 0.6935 Acc: 0.8396\n",
      "\n",
      "Epoch: 145/500\n",
      "------------------------------\n",
      "train Loss: 0.7833 Acc: 0.7862\n",
      "val Loss: 0.9204 Acc: 0.8175\n",
      "\n",
      "Epoch: 146/500\n",
      "------------------------------\n",
      "train Loss: 0.8201 Acc: 0.7824\n",
      "val Loss: 0.9044 Acc: 0.8238\n",
      "\n",
      "Epoch: 147/500\n",
      "------------------------------\n",
      "train Loss: 0.8030 Acc: 0.7853\n",
      "val Loss: 0.8036 Acc: 0.8385\n",
      "\n",
      "Epoch: 148/500\n",
      "------------------------------\n",
      "train Loss: 0.8504 Acc: 0.7780\n",
      "val Loss: 0.9044 Acc: 0.8075\n",
      "\n",
      "Epoch: 149/500\n",
      "------------------------------\n",
      "train Loss: 0.8315 Acc: 0.7786\n",
      "val Loss: 0.5901 Acc: 0.8611\n",
      "\n",
      "Epoch: 150/500\n",
      "------------------------------\n",
      "train Loss: 0.8070 Acc: 0.7817\n",
      "val Loss: 0.6222 Acc: 0.8469\n",
      "\n",
      "Epoch: 151/500\n",
      "------------------------------\n",
      "train Loss: 0.8063 Acc: 0.7821\n",
      "val Loss: 0.7008 Acc: 0.8459\n",
      "\n",
      "Epoch: 152/500\n",
      "------------------------------\n",
      "train Loss: 0.8134 Acc: 0.7832\n",
      "val Loss: 0.5016 Acc: 0.8617\n",
      "\n",
      "Epoch: 153/500\n",
      "------------------------------\n",
      "train Loss: 0.8058 Acc: 0.7784\n",
      "val Loss: 0.7690 Acc: 0.8169\n",
      "\n",
      "Epoch: 154/500\n",
      "------------------------------\n",
      "train Loss: 0.8268 Acc: 0.7786\n",
      "val Loss: 0.6124 Acc: 0.8564\n",
      "\n",
      "Epoch: 155/500\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.7982 Acc: 0.7830\n",
      "val Loss: 0.5649 Acc: 0.8585\n",
      "\n",
      "Epoch: 156/500\n",
      "------------------------------\n",
      "train Loss: 0.8243 Acc: 0.7781\n",
      "val Loss: 0.5277 Acc: 0.8695\n",
      "\n",
      "Epoch: 157/500\n",
      "------------------------------\n",
      "train Loss: 0.8344 Acc: 0.7818\n",
      "val Loss: 0.5580 Acc: 0.8701\n",
      "\n",
      "Epoch: 158/500\n",
      "------------------------------\n",
      "train Loss: 0.8015 Acc: 0.7776\n",
      "val Loss: 0.7166 Acc: 0.8396\n",
      "\n",
      "Epoch: 159/500\n",
      "------------------------------\n",
      "train Loss: 0.8029 Acc: 0.7848\n",
      "val Loss: 0.8127 Acc: 0.8217\n",
      "\n",
      "Epoch: 160/500\n",
      "------------------------------\n",
      "train Loss: 0.9995 Acc: 0.7661\n",
      "val Loss: 0.8415 Acc: 0.8164\n",
      "\n",
      "Epoch: 161/500\n",
      "------------------------------\n",
      "train Loss: 0.7851 Acc: 0.7869\n",
      "val Loss: 0.4331 Acc: 0.8869\n",
      "\n",
      "Epoch: 162/500\n",
      "------------------------------\n",
      "train Loss: 0.8217 Acc: 0.7787\n",
      "val Loss: 0.7338 Acc: 0.8290\n",
      "\n",
      "Epoch: 163/500\n",
      "------------------------------\n",
      "train Loss: 0.8164 Acc: 0.7792\n",
      "val Loss: 0.6647 Acc: 0.8396\n",
      "\n",
      "Epoch: 164/500\n",
      "------------------------------\n",
      "train Loss: 0.8018 Acc: 0.7813\n",
      "val Loss: 0.4049 Acc: 0.8785\n",
      "\n",
      "Epoch: 165/500\n",
      "------------------------------\n",
      "train Loss: 0.7944 Acc: 0.7820\n",
      "val Loss: 0.7103 Acc: 0.8380\n",
      "\n",
      "Epoch: 166/500\n",
      "------------------------------\n",
      "train Loss: 0.8205 Acc: 0.7769\n",
      "val Loss: 0.5346 Acc: 0.8659\n",
      "\n",
      "Epoch: 167/500\n",
      "------------------------------\n",
      "train Loss: 0.7898 Acc: 0.7859\n",
      "val Loss: 0.6292 Acc: 0.8432\n",
      "\n",
      "Epoch: 168/500\n",
      "------------------------------\n",
      "train Loss: 0.8052 Acc: 0.7828\n",
      "val Loss: 0.6643 Acc: 0.8417\n",
      "\n",
      "Epoch: 169/500\n",
      "------------------------------\n",
      "train Loss: 0.8350 Acc: 0.7754\n",
      "val Loss: 0.5632 Acc: 0.8553\n",
      "\n",
      "Epoch: 170/500\n",
      "------------------------------\n",
      "train Loss: 0.8179 Acc: 0.7828\n",
      "val Loss: 0.6583 Acc: 0.8443\n",
      "\n",
      "Epoch: 171/500\n",
      "------------------------------\n",
      "train Loss: 0.8121 Acc: 0.7850\n",
      "val Loss: 0.6836 Acc: 0.8427\n",
      "\n",
      "Epoch: 172/500\n",
      "------------------------------\n",
      "train Loss: 0.7865 Acc: 0.7898\n",
      "val Loss: 0.6718 Acc: 0.8464\n",
      "\n",
      "Epoch: 173/500\n",
      "------------------------------\n",
      "train Loss: 0.8481 Acc: 0.7737\n",
      "val Loss: 0.5196 Acc: 0.8685\n",
      "\n",
      "Epoch: 174/500\n",
      "------------------------------\n",
      "train Loss: 0.8338 Acc: 0.7796\n",
      "val Loss: 0.5833 Acc: 0.8474\n",
      "\n",
      "Epoch: 175/500\n",
      "------------------------------\n",
      "train Loss: 0.8148 Acc: 0.7843\n",
      "val Loss: 0.7470 Acc: 0.8264\n",
      "\n",
      "Epoch: 176/500\n",
      "------------------------------\n",
      "train Loss: 0.7991 Acc: 0.7855\n",
      "val Loss: 0.5714 Acc: 0.8595\n",
      "\n",
      "Epoch: 177/500\n",
      "------------------------------\n",
      "train Loss: 0.7729 Acc: 0.7903\n",
      "val Loss: 0.5033 Acc: 0.8701\n",
      "\n",
      "Epoch: 178/500\n",
      "------------------------------\n",
      "train Loss: 0.8160 Acc: 0.7778\n",
      "val Loss: 0.5987 Acc: 0.8506\n",
      "\n",
      "Epoch: 179/500\n",
      "------------------------------\n",
      "train Loss: 0.7892 Acc: 0.7836\n",
      "val Loss: 0.5403 Acc: 0.8643\n",
      "\n",
      "Epoch: 180/500\n",
      "------------------------------\n",
      "train Loss: 0.7699 Acc: 0.7874\n",
      "val Loss: 0.6678 Acc: 0.8485\n",
      "\n",
      "Epoch: 181/500\n",
      "------------------------------\n",
      "train Loss: 0.8043 Acc: 0.7812\n",
      "val Loss: 0.7001 Acc: 0.8375\n",
      "\n",
      "Epoch: 182/500\n",
      "------------------------------\n",
      "train Loss: 0.8111 Acc: 0.7806\n",
      "val Loss: 0.6371 Acc: 0.8485\n",
      "\n",
      "Epoch: 183/500\n",
      "------------------------------\n",
      "train Loss: 0.7775 Acc: 0.7888\n",
      "val Loss: 0.5554 Acc: 0.8559\n",
      "\n",
      "Epoch: 184/500\n",
      "------------------------------\n",
      "train Loss: 0.8308 Acc: 0.7765\n",
      "val Loss: 0.6292 Acc: 0.8480\n",
      "\n",
      "Epoch: 185/500\n",
      "------------------------------\n",
      "train Loss: 0.7881 Acc: 0.7881\n",
      "val Loss: 0.8474 Acc: 0.8138\n",
      "\n",
      "Epoch: 186/500\n",
      "------------------------------\n",
      "train Loss: 0.8460 Acc: 0.7787\n",
      "val Loss: 0.9592 Acc: 0.8090\n",
      "\n",
      "Epoch: 187/500\n",
      "------------------------------\n",
      "train Loss: 0.8361 Acc: 0.7801\n",
      "val Loss: 0.6682 Acc: 0.8422\n",
      "\n",
      "Epoch: 188/500\n",
      "------------------------------\n",
      "train Loss: 0.7984 Acc: 0.7841\n",
      "val Loss: 0.7589 Acc: 0.8327\n",
      "\n",
      "Epoch: 189/500\n",
      "------------------------------\n",
      "train Loss: 0.8493 Acc: 0.7752\n",
      "val Loss: 0.7929 Acc: 0.8127\n",
      "\n",
      "Epoch: 190/500\n",
      "------------------------------\n",
      "train Loss: 0.7826 Acc: 0.7906\n",
      "val Loss: 0.6486 Acc: 0.8459\n",
      "\n",
      "Epoch: 191/500\n",
      "------------------------------\n",
      "train Loss: 0.8025 Acc: 0.7826\n",
      "val Loss: 0.6378 Acc: 0.8417\n",
      "\n",
      "Epoch: 192/500\n",
      "------------------------------\n",
      "train Loss: 0.7637 Acc: 0.7910\n",
      "val Loss: 0.5606 Acc: 0.8643\n",
      "\n",
      "Epoch: 193/500\n",
      "------------------------------\n",
      "train Loss: 0.8025 Acc: 0.7801\n",
      "val Loss: 0.6716 Acc: 0.8317\n",
      "\n",
      "Epoch: 194/500\n",
      "------------------------------\n",
      "train Loss: 0.8000 Acc: 0.7805\n",
      "val Loss: 0.4932 Acc: 0.8664\n",
      "\n",
      "Epoch: 195/500\n",
      "------------------------------\n",
      "train Loss: 0.7889 Acc: 0.7783\n",
      "val Loss: 0.6761 Acc: 0.8511\n",
      "\n",
      "Epoch: 196/500\n",
      "------------------------------\n",
      "train Loss: 0.7754 Acc: 0.7882\n",
      "val Loss: 0.7494 Acc: 0.8322\n",
      "\n",
      "Epoch: 197/500\n",
      "------------------------------\n",
      "train Loss: 0.8294 Acc: 0.7761\n",
      "val Loss: 0.5453 Acc: 0.8553\n",
      "\n",
      "Epoch: 198/500\n",
      "------------------------------\n",
      "train Loss: 0.8120 Acc: 0.7843\n",
      "val Loss: 0.8117 Acc: 0.8133\n",
      "\n",
      "Epoch: 199/500\n",
      "------------------------------\n",
      "train Loss: 0.8098 Acc: 0.7797\n",
      "val Loss: 0.7718 Acc: 0.8206\n",
      "\n",
      "Epoch: 200/500\n",
      "------------------------------\n",
      "train Loss: 0.8172 Acc: 0.7789\n",
      "val Loss: 0.5911 Acc: 0.8569\n",
      "\n",
      "Epoch: 201/500\n",
      "------------------------------\n",
      "train Loss: 0.8658 Acc: 0.7736\n",
      "val Loss: 0.7299 Acc: 0.8343\n",
      "\n",
      "Epoch: 202/500\n",
      "------------------------------\n",
      "train Loss: 0.8256 Acc: 0.7841\n",
      "val Loss: 0.6290 Acc: 0.8496\n",
      "\n",
      "Epoch: 203/500\n",
      "------------------------------\n",
      "train Loss: 0.8043 Acc: 0.7837\n",
      "val Loss: 0.7246 Acc: 0.8390\n",
      "\n",
      "Epoch: 204/500\n",
      "------------------------------\n",
      "train Loss: 0.7846 Acc: 0.7870\n",
      "val Loss: 0.6244 Acc: 0.8485\n",
      "\n",
      "Epoch: 205/500\n",
      "------------------------------\n",
      "train Loss: 0.8037 Acc: 0.7828\n",
      "val Loss: 0.6161 Acc: 0.8559\n",
      "\n",
      "Epoch: 206/500\n",
      "------------------------------\n",
      "train Loss: 0.8215 Acc: 0.7792\n",
      "val Loss: 0.4061 Acc: 0.8880\n",
      "\n",
      "Epoch: 207/500\n",
      "------------------------------\n",
      "train Loss: 0.8287 Acc: 0.7815\n",
      "val Loss: 0.6515 Acc: 0.8496\n",
      "\n",
      "Epoch: 208/500\n",
      "------------------------------\n",
      "train Loss: 0.8044 Acc: 0.7877\n",
      "val Loss: 0.8337 Acc: 0.8169\n",
      "\n",
      "Epoch: 209/500\n",
      "------------------------------\n",
      "train Loss: 0.8746 Acc: 0.7727\n",
      "val Loss: 0.6202 Acc: 0.8401\n",
      "\n",
      "Epoch: 210/500\n",
      "------------------------------\n",
      "train Loss: 0.8250 Acc: 0.7795\n",
      "val Loss: 0.6956 Acc: 0.8506\n",
      "\n",
      "Epoch: 211/500\n",
      "------------------------------\n",
      "train Loss: 0.8558 Acc: 0.7769\n",
      "val Loss: 0.6826 Acc: 0.8396\n",
      "\n",
      "Epoch: 212/500\n",
      "------------------------------\n",
      "train Loss: 0.8598 Acc: 0.7752\n",
      "val Loss: 0.7463 Acc: 0.8290\n",
      "\n",
      "Epoch: 213/500\n",
      "------------------------------\n",
      "train Loss: 0.8070 Acc: 0.7818\n",
      "val Loss: 0.6450 Acc: 0.8432\n",
      "\n",
      "Epoch: 214/500\n",
      "------------------------------\n",
      "train Loss: 0.8622 Acc: 0.7748\n",
      "val Loss: 0.7463 Acc: 0.8390\n",
      "\n",
      "Epoch: 215/500\n",
      "------------------------------\n",
      "train Loss: 0.8313 Acc: 0.7817\n",
      "val Loss: 0.7973 Acc: 0.8243\n",
      "\n",
      "Epoch: 216/500\n",
      "------------------------------\n",
      "train Loss: 0.8565 Acc: 0.7801\n",
      "val Loss: 0.6061 Acc: 0.8506\n",
      "\n",
      "Epoch: 217/500\n",
      "------------------------------\n",
      "train Loss: 0.7905 Acc: 0.7876\n",
      "val Loss: 0.5957 Acc: 0.8490\n",
      "\n",
      "Epoch: 218/500\n",
      "------------------------------\n",
      "train Loss: 0.8428 Acc: 0.7768\n",
      "val Loss: 0.6942 Acc: 0.8317\n",
      "\n",
      "Epoch: 219/500\n",
      "------------------------------\n",
      "train Loss: 0.8322 Acc: 0.7776\n",
      "val Loss: 0.6867 Acc: 0.8422\n",
      "\n",
      "Epoch: 220/500\n",
      "------------------------------\n",
      "train Loss: 0.7969 Acc: 0.7857\n",
      "val Loss: 0.5918 Acc: 0.8564\n",
      "\n",
      "Epoch: 221/500\n",
      "------------------------------\n",
      "train Loss: 0.8346 Acc: 0.7799\n",
      "val Loss: 0.5026 Acc: 0.8674\n",
      "\n",
      "Epoch: 222/500\n",
      "------------------------------\n",
      "train Loss: 0.8051 Acc: 0.7834\n",
      "val Loss: 1.0404 Acc: 0.7859\n",
      "\n",
      "Epoch: 223/500\n",
      "------------------------------\n",
      "train Loss: 0.7752 Acc: 0.7912\n",
      "val Loss: 0.8937 Acc: 0.8012\n",
      "\n",
      "Epoch: 224/500\n",
      "------------------------------\n",
      "train Loss: 0.8675 Acc: 0.7735\n",
      "val Loss: 0.7603 Acc: 0.8369\n",
      "\n",
      "Epoch: 225/500\n",
      "------------------------------\n",
      "train Loss: 0.8456 Acc: 0.7784\n",
      "val Loss: 0.6675 Acc: 0.8469\n",
      "\n",
      "Epoch: 226/500\n",
      "------------------------------\n",
      "train Loss: 0.8370 Acc: 0.7795\n",
      "val Loss: 0.7796 Acc: 0.8317\n",
      "\n",
      "Epoch: 227/500\n",
      "------------------------------\n",
      "train Loss: 0.8113 Acc: 0.7834\n",
      "val Loss: 0.7046 Acc: 0.8385\n",
      "\n",
      "Epoch: 228/500\n",
      "------------------------------\n",
      "train Loss: 0.8869 Acc: 0.7765\n",
      "val Loss: 1.1077 Acc: 0.7896\n",
      "\n",
      "Epoch: 229/500\n",
      "------------------------------\n",
      "train Loss: 0.8116 Acc: 0.7786\n",
      "val Loss: 0.7608 Acc: 0.8122\n",
      "\n",
      "Epoch: 230/500\n",
      "------------------------------\n",
      "train Loss: 0.8464 Acc: 0.7788\n",
      "val Loss: 0.5240 Acc: 0.8659\n",
      "\n",
      "Epoch: 231/500\n",
      "------------------------------\n",
      "train Loss: 0.8268 Acc: 0.7828\n",
      "val Loss: 0.5829 Acc: 0.8543\n",
      "\n",
      "Epoch: 232/500\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.8143 Acc: 0.7829\n",
      "val Loss: 0.8373 Acc: 0.8206\n",
      "\n",
      "Epoch: 233/500\n",
      "------------------------------\n",
      "train Loss: 0.7774 Acc: 0.7902\n",
      "val Loss: 0.8862 Acc: 0.8143\n",
      "\n",
      "Epoch: 234/500\n",
      "------------------------------\n",
      "train Loss: 0.8204 Acc: 0.7814\n",
      "val Loss: 0.4517 Acc: 0.8822\n",
      "\n",
      "Epoch: 235/500\n",
      "------------------------------\n",
      "train Loss: 0.8065 Acc: 0.7813\n",
      "val Loss: 0.6883 Acc: 0.8306\n",
      "\n",
      "Epoch: 236/500\n",
      "------------------------------\n",
      "train Loss: 0.8244 Acc: 0.7813\n",
      "val Loss: 0.5466 Acc: 0.8590\n",
      "\n",
      "Epoch: 237/500\n",
      "------------------------------\n",
      "train Loss: 0.7944 Acc: 0.7837\n",
      "val Loss: 0.5823 Acc: 0.8517\n",
      "\n",
      "Epoch: 238/500\n",
      "------------------------------\n",
      "train Loss: 0.8216 Acc: 0.7824\n",
      "val Loss: 0.5438 Acc: 0.8690\n",
      "\n",
      "Epoch: 239/500\n",
      "------------------------------\n",
      "train Loss: 0.8089 Acc: 0.7780\n",
      "val Loss: 0.5924 Acc: 0.8496\n",
      "\n",
      "Epoch: 240/500\n",
      "------------------------------\n",
      "train Loss: 0.7747 Acc: 0.7895\n",
      "val Loss: 0.5953 Acc: 0.8553\n",
      "\n",
      "Epoch: 241/500\n",
      "------------------------------\n",
      "train Loss: 0.7788 Acc: 0.7838\n",
      "val Loss: 0.6250 Acc: 0.8459\n",
      "\n",
      "Epoch: 242/500\n",
      "------------------------------\n",
      "train Loss: 0.8710 Acc: 0.7767\n",
      "val Loss: 0.4511 Acc: 0.8738\n",
      "\n",
      "Epoch: 243/500\n",
      "------------------------------\n",
      "train Loss: 0.8149 Acc: 0.7852\n",
      "val Loss: 0.7978 Acc: 0.8301\n",
      "\n",
      "Epoch: 244/500\n",
      "------------------------------\n",
      "train Loss: 0.8004 Acc: 0.7896\n",
      "val Loss: 0.7461 Acc: 0.8206\n",
      "\n",
      "Epoch: 245/500\n",
      "------------------------------\n",
      "train Loss: 0.8227 Acc: 0.7786\n",
      "val Loss: 0.6522 Acc: 0.8480\n",
      "\n",
      "Epoch: 246/500\n",
      "------------------------------\n",
      "train Loss: 0.8259 Acc: 0.7847\n",
      "val Loss: 0.4941 Acc: 0.8669\n",
      "\n",
      "Epoch: 247/500\n",
      "------------------------------\n",
      "train Loss: 0.8054 Acc: 0.7816\n",
      "val Loss: 0.6152 Acc: 0.8480\n",
      "\n",
      "Epoch: 248/500\n",
      "------------------------------\n",
      "train Loss: 0.8279 Acc: 0.7820\n",
      "val Loss: 0.5767 Acc: 0.8580\n",
      "\n",
      "Epoch: 249/500\n",
      "------------------------------\n",
      "train Loss: 0.7931 Acc: 0.7846\n",
      "val Loss: 0.4907 Acc: 0.8690\n",
      "\n",
      "Epoch: 250/500\n",
      "------------------------------\n",
      "train Loss: 0.8066 Acc: 0.7818\n",
      "val Loss: 0.6393 Acc: 0.8585\n",
      "\n",
      "Epoch: 251/500\n",
      "------------------------------\n",
      "train Loss: 0.8266 Acc: 0.7792\n",
      "val Loss: 0.6714 Acc: 0.8396\n",
      "\n",
      "Epoch: 252/500\n",
      "------------------------------\n",
      "train Loss: 0.7841 Acc: 0.7863\n",
      "val Loss: 0.5280 Acc: 0.8632\n",
      "\n",
      "Epoch: 253/500\n",
      "------------------------------\n",
      "train Loss: 0.7725 Acc: 0.7868\n",
      "val Loss: 0.6731 Acc: 0.8301\n",
      "\n",
      "Epoch: 254/500\n",
      "------------------------------\n",
      "train Loss: 0.7900 Acc: 0.7856\n",
      "val Loss: 0.4158 Acc: 0.8843\n",
      "\n",
      "Epoch: 255/500\n",
      "------------------------------\n",
      "train Loss: 0.8305 Acc: 0.7805\n",
      "val Loss: 0.7043 Acc: 0.8311\n",
      "\n",
      "Epoch: 256/500\n",
      "------------------------------\n",
      "train Loss: 0.8050 Acc: 0.7853\n",
      "val Loss: 0.6099 Acc: 0.8527\n",
      "\n",
      "Epoch: 257/500\n",
      "------------------------------\n",
      "train Loss: 0.8931 Acc: 0.7738\n",
      "val Loss: 0.8391 Acc: 0.8432\n",
      "\n",
      "Epoch: 258/500\n",
      "------------------------------\n",
      "train Loss: 0.8293 Acc: 0.7851\n",
      "val Loss: 0.7275 Acc: 0.8269\n",
      "\n",
      "Epoch: 259/500\n",
      "------------------------------\n",
      "train Loss: 0.8231 Acc: 0.7805\n",
      "val Loss: 0.6831 Acc: 0.8417\n",
      "\n",
      "Epoch: 260/500\n",
      "------------------------------\n",
      "train Loss: 0.8140 Acc: 0.7856\n",
      "val Loss: 0.7100 Acc: 0.8453\n",
      "\n",
      "Epoch: 261/500\n",
      "------------------------------\n",
      "train Loss: 0.7926 Acc: 0.7873\n",
      "val Loss: 0.5780 Acc: 0.8506\n",
      "\n",
      "Epoch: 262/500\n",
      "------------------------------\n",
      "train Loss: 0.8232 Acc: 0.7828\n",
      "val Loss: 1.0403 Acc: 0.8006\n",
      "\n",
      "Epoch: 263/500\n",
      "------------------------------\n",
      "train Loss: 0.7924 Acc: 0.7868\n",
      "val Loss: 0.5919 Acc: 0.8622\n",
      "\n",
      "Epoch: 264/500\n",
      "------------------------------\n",
      "train Loss: 0.8437 Acc: 0.7824\n",
      "val Loss: 0.6957 Acc: 0.8417\n",
      "\n",
      "Epoch: 265/500\n",
      "------------------------------\n",
      "train Loss: 0.8182 Acc: 0.7819\n",
      "val Loss: 0.5752 Acc: 0.8627\n",
      "\n",
      "Epoch: 266/500\n",
      "------------------------------\n",
      "train Loss: 0.8069 Acc: 0.7829\n",
      "val Loss: 0.6023 Acc: 0.8490\n",
      "\n",
      "Epoch: 267/500\n",
      "------------------------------\n",
      "train Loss: 0.8235 Acc: 0.7820\n",
      "val Loss: 0.6655 Acc: 0.8448\n",
      "\n",
      "Epoch: 268/500\n",
      "------------------------------\n",
      "train Loss: 0.7937 Acc: 0.7869\n",
      "val Loss: 0.6473 Acc: 0.8511\n",
      "\n",
      "Epoch: 269/500\n",
      "------------------------------\n",
      "train Loss: 0.7969 Acc: 0.7879\n",
      "val Loss: 0.5828 Acc: 0.8548\n",
      "\n",
      "Epoch: 270/500\n",
      "------------------------------\n",
      "train Loss: 0.8080 Acc: 0.7877\n",
      "val Loss: 0.7873 Acc: 0.8185\n",
      "\n",
      "Epoch: 271/500\n",
      "------------------------------\n",
      "train Loss: 0.8180 Acc: 0.7847\n",
      "val Loss: 0.8196 Acc: 0.8169\n",
      "\n",
      "Epoch: 272/500\n",
      "------------------------------\n",
      "train Loss: 0.7589 Acc: 0.7911\n",
      "val Loss: 0.7927 Acc: 0.8285\n",
      "\n",
      "Epoch: 273/500\n",
      "------------------------------\n",
      "train Loss: 0.8089 Acc: 0.7816\n",
      "val Loss: 0.8903 Acc: 0.8069\n",
      "\n",
      "Epoch: 274/500\n",
      "------------------------------\n",
      "train Loss: 0.8309 Acc: 0.7796\n",
      "val Loss: 0.5320 Acc: 0.8601\n",
      "\n",
      "Epoch: 275/500\n",
      "------------------------------\n",
      "train Loss: 0.7845 Acc: 0.7919\n",
      "val Loss: 0.6611 Acc: 0.8422\n",
      "\n",
      "Epoch: 276/500\n",
      "------------------------------\n",
      "train Loss: 0.8495 Acc: 0.7765\n",
      "val Loss: 0.3875 Acc: 0.8885\n",
      "\n",
      "Epoch: 277/500\n",
      "------------------------------\n",
      "train Loss: 0.8103 Acc: 0.7815\n",
      "val Loss: 0.5872 Acc: 0.8517\n",
      "\n",
      "Epoch: 278/500\n",
      "------------------------------\n",
      "train Loss: 0.8252 Acc: 0.7797\n",
      "val Loss: 0.7935 Acc: 0.8248\n",
      "\n",
      "Epoch: 279/500\n",
      "------------------------------\n",
      "train Loss: 0.7979 Acc: 0.7899\n",
      "val Loss: 0.4991 Acc: 0.8727\n",
      "\n",
      "Epoch: 280/500\n",
      "------------------------------\n",
      "train Loss: 0.7919 Acc: 0.7850\n",
      "val Loss: 0.9281 Acc: 0.8133\n",
      "\n",
      "Epoch: 281/500\n",
      "------------------------------\n",
      "train Loss: 0.7790 Acc: 0.7891\n",
      "val Loss: 0.4250 Acc: 0.8743\n",
      "\n",
      "Epoch: 282/500\n",
      "------------------------------\n",
      "train Loss: 0.7732 Acc: 0.7879\n",
      "val Loss: 0.8009 Acc: 0.8201\n",
      "\n",
      "Epoch: 283/500\n",
      "------------------------------\n",
      "train Loss: 0.7731 Acc: 0.7898\n",
      "val Loss: 0.5707 Acc: 0.8590\n",
      "\n",
      "Epoch: 284/500\n",
      "------------------------------\n",
      "train Loss: 0.7709 Acc: 0.7868\n",
      "val Loss: 0.5717 Acc: 0.8548\n",
      "\n",
      "Epoch: 285/500\n",
      "------------------------------\n",
      "train Loss: 0.8231 Acc: 0.7794\n",
      "val Loss: 0.7101 Acc: 0.8480\n",
      "\n",
      "Epoch: 286/500\n",
      "------------------------------\n",
      "train Loss: 0.8263 Acc: 0.7828\n",
      "val Loss: 0.5388 Acc: 0.8595\n",
      "\n",
      "Epoch: 287/500\n",
      "------------------------------\n",
      "train Loss: 0.8119 Acc: 0.7870\n",
      "val Loss: 0.5921 Acc: 0.8517\n",
      "\n",
      "Epoch: 288/500\n",
      "------------------------------\n",
      "train Loss: 0.7972 Acc: 0.7868\n",
      "val Loss: 0.5827 Acc: 0.8548\n",
      "\n",
      "Epoch: 289/500\n",
      "------------------------------\n",
      "train Loss: 0.7928 Acc: 0.7857\n",
      "val Loss: 0.6410 Acc: 0.8459\n",
      "\n",
      "Epoch: 290/500\n",
      "------------------------------\n",
      "train Loss: 0.7683 Acc: 0.7895\n",
      "val Loss: 0.5564 Acc: 0.8527\n",
      "\n",
      "Epoch: 291/500\n",
      "------------------------------\n",
      "train Loss: 0.8053 Acc: 0.7853\n",
      "val Loss: 0.6240 Acc: 0.8506\n",
      "\n",
      "Epoch: 292/500\n",
      "------------------------------\n",
      "train Loss: 0.8013 Acc: 0.7828\n",
      "val Loss: 0.4774 Acc: 0.8685\n",
      "\n",
      "Epoch: 293/500\n",
      "------------------------------\n",
      "train Loss: 0.8104 Acc: 0.7841\n",
      "val Loss: 0.6040 Acc: 0.8564\n",
      "\n",
      "Epoch: 294/500\n",
      "------------------------------\n",
      "train Loss: 0.8125 Acc: 0.7862\n",
      "val Loss: 0.6638 Acc: 0.8501\n",
      "\n",
      "Epoch: 295/500\n",
      "------------------------------\n",
      "train Loss: 0.8099 Acc: 0.7860\n",
      "val Loss: 0.6853 Acc: 0.8469\n",
      "\n",
      "Epoch: 296/500\n",
      "------------------------------\n",
      "train Loss: 0.8377 Acc: 0.7808\n",
      "val Loss: 0.7013 Acc: 0.8385\n",
      "\n",
      "Epoch: 297/500\n",
      "------------------------------\n",
      "train Loss: 0.8305 Acc: 0.7791\n",
      "val Loss: 0.4885 Acc: 0.8806\n",
      "\n",
      "Epoch: 298/500\n",
      "------------------------------\n",
      "train Loss: 0.8555 Acc: 0.7776\n",
      "val Loss: 1.0503 Acc: 0.8148\n",
      "\n",
      "Epoch: 299/500\n",
      "------------------------------\n",
      "train Loss: 0.8524 Acc: 0.7811\n",
      "val Loss: 0.5900 Acc: 0.8617\n",
      "\n",
      "Epoch: 300/500\n",
      "------------------------------\n",
      "train Loss: 0.8167 Acc: 0.7836\n",
      "val Loss: 0.8318 Acc: 0.8211\n",
      "\n",
      "Epoch: 301/500\n",
      "------------------------------\n",
      "train Loss: 0.8490 Acc: 0.7790\n",
      "val Loss: 0.7442 Acc: 0.8327\n",
      "\n",
      "Epoch: 302/500\n",
      "------------------------------\n",
      "train Loss: 0.7674 Acc: 0.7912\n",
      "val Loss: 0.5205 Acc: 0.8564\n",
      "\n",
      "Epoch: 303/500\n",
      "------------------------------\n",
      "train Loss: 0.7826 Acc: 0.7884\n",
      "val Loss: 0.5816 Acc: 0.8543\n",
      "\n",
      "Epoch: 304/500\n",
      "------------------------------\n",
      "train Loss: 0.7921 Acc: 0.7857\n",
      "val Loss: 0.5769 Acc: 0.8574\n",
      "\n",
      "Epoch: 305/500\n",
      "------------------------------\n",
      "train Loss: 0.8097 Acc: 0.7839\n",
      "val Loss: 0.6877 Acc: 0.8375\n",
      "\n",
      "Epoch: 306/500\n",
      "------------------------------\n",
      "train Loss: 0.8183 Acc: 0.7811\n",
      "val Loss: 1.0442 Acc: 0.7959\n",
      "\n",
      "Epoch: 307/500\n",
      "------------------------------\n",
      "train Loss: 0.8396 Acc: 0.7790\n",
      "val Loss: 0.7949 Acc: 0.8338\n",
      "\n",
      "Epoch: 308/500\n",
      "------------------------------\n",
      "train Loss: 0.8868 Acc: 0.7735\n",
      "val Loss: 0.5829 Acc: 0.8538\n",
      "\n",
      "Epoch: 309/500\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.8012 Acc: 0.7830\n",
      "val Loss: 0.5670 Acc: 0.8701\n",
      "\n",
      "Epoch: 310/500\n",
      "------------------------------\n",
      "train Loss: 0.8808 Acc: 0.7790\n",
      "val Loss: 0.7333 Acc: 0.8369\n",
      "\n",
      "Epoch: 311/500\n",
      "------------------------------\n",
      "train Loss: 0.8144 Acc: 0.7856\n",
      "val Loss: 0.6972 Acc: 0.8438\n",
      "\n",
      "Epoch: 312/500\n",
      "------------------------------\n",
      "train Loss: 0.7682 Acc: 0.7906\n",
      "val Loss: 0.5302 Acc: 0.8601\n",
      "\n",
      "Epoch: 313/500\n",
      "------------------------------\n",
      "train Loss: 0.8112 Acc: 0.7823\n",
      "val Loss: 0.7478 Acc: 0.8406\n",
      "\n",
      "Epoch: 314/500\n",
      "------------------------------\n",
      "train Loss: 0.8063 Acc: 0.7863\n",
      "val Loss: 0.7900 Acc: 0.8206\n",
      "\n",
      "Epoch: 315/500\n",
      "------------------------------\n",
      "train Loss: 0.7803 Acc: 0.7933\n",
      "val Loss: 0.5109 Acc: 0.8706\n",
      "\n",
      "Epoch: 316/500\n",
      "------------------------------\n",
      "train Loss: 0.8171 Acc: 0.7867\n",
      "val Loss: 0.8936 Acc: 0.8012\n",
      "\n",
      "Epoch: 317/500\n",
      "------------------------------\n",
      "train Loss: 0.8092 Acc: 0.7875\n",
      "val Loss: 0.6976 Acc: 0.8401\n",
      "\n",
      "Epoch: 318/500\n",
      "------------------------------\n",
      "train Loss: 0.7871 Acc: 0.7905\n",
      "val Loss: 0.6272 Acc: 0.8469\n",
      "\n",
      "Epoch: 319/500\n",
      "------------------------------\n",
      "train Loss: 0.7660 Acc: 0.7892\n",
      "val Loss: 0.6323 Acc: 0.8527\n",
      "\n",
      "Epoch: 320/500\n",
      "------------------------------\n",
      "train Loss: 0.8220 Acc: 0.7800\n",
      "val Loss: 0.6019 Acc: 0.8527\n",
      "\n",
      "Epoch: 321/500\n",
      "------------------------------\n",
      "train Loss: 0.7873 Acc: 0.7874\n",
      "val Loss: 0.7746 Acc: 0.8380\n",
      "\n",
      "Epoch: 322/500\n",
      "------------------------------\n",
      "train Loss: 0.8010 Acc: 0.7853\n",
      "val Loss: 0.8440 Acc: 0.8112\n",
      "\n",
      "Epoch: 323/500\n",
      "------------------------------\n",
      "train Loss: 0.8041 Acc: 0.7856\n",
      "val Loss: 0.6942 Acc: 0.8343\n",
      "\n",
      "Epoch: 324/500\n",
      "------------------------------\n",
      "train Loss: 0.7849 Acc: 0.7850\n",
      "val Loss: 0.8854 Acc: 0.8133\n",
      "\n",
      "Epoch: 325/500\n",
      "------------------------------\n",
      "train Loss: 0.8357 Acc: 0.7821\n",
      "val Loss: 0.5127 Acc: 0.8632\n",
      "\n",
      "Epoch: 326/500\n",
      "------------------------------\n",
      "train Loss: 0.8089 Acc: 0.7855\n",
      "val Loss: 0.7676 Acc: 0.8332\n",
      "\n",
      "Epoch: 327/500\n",
      "------------------------------\n",
      "train Loss: 0.8719 Acc: 0.7776\n",
      "val Loss: 1.1254 Acc: 0.7817\n",
      "\n",
      "Epoch: 328/500\n",
      "------------------------------\n",
      "train Loss: 0.8376 Acc: 0.7817\n",
      "val Loss: 0.6510 Acc: 0.8390\n",
      "\n",
      "Epoch: 329/500\n",
      "------------------------------\n",
      "train Loss: 0.7862 Acc: 0.7893\n",
      "val Loss: 0.6332 Acc: 0.8564\n",
      "\n",
      "Epoch: 330/500\n",
      "------------------------------\n",
      "train Loss: 0.8971 Acc: 0.7752\n",
      "val Loss: 0.4513 Acc: 0.8837\n",
      "\n",
      "Epoch: 331/500\n",
      "------------------------------\n",
      "train Loss: 0.7867 Acc: 0.7962\n",
      "val Loss: 0.5117 Acc: 0.8690\n",
      "\n",
      "Epoch: 332/500\n",
      "------------------------------\n",
      "train Loss: 0.8100 Acc: 0.7828\n",
      "val Loss: 0.7159 Acc: 0.8411\n",
      "\n",
      "Epoch: 333/500\n",
      "------------------------------\n",
      "train Loss: 0.7798 Acc: 0.7892\n",
      "val Loss: 0.6458 Acc: 0.8469\n",
      "\n",
      "Epoch: 334/500\n",
      "------------------------------\n",
      "train Loss: 0.7807 Acc: 0.7893\n",
      "val Loss: 0.4622 Acc: 0.8738\n",
      "\n",
      "Epoch: 335/500\n",
      "------------------------------\n",
      "train Loss: 0.8089 Acc: 0.7861\n",
      "val Loss: 0.6033 Acc: 0.8548\n",
      "\n",
      "Epoch: 336/500\n",
      "------------------------------\n",
      "train Loss: 0.9279 Acc: 0.7708\n",
      "val Loss: 1.0188 Acc: 0.7969\n",
      "\n",
      "Epoch: 337/500\n",
      "------------------------------\n",
      "train Loss: 0.7879 Acc: 0.7892\n",
      "val Loss: 0.5189 Acc: 0.8695\n",
      "\n",
      "Epoch: 338/500\n",
      "------------------------------\n",
      "train Loss: 0.7749 Acc: 0.7887\n",
      "val Loss: 0.4861 Acc: 0.8843\n",
      "\n",
      "Epoch: 339/500\n",
      "------------------------------\n",
      "train Loss: 0.8237 Acc: 0.7830\n",
      "val Loss: 0.7101 Acc: 0.8317\n",
      "\n",
      "Epoch: 340/500\n",
      "------------------------------\n",
      "train Loss: 0.7873 Acc: 0.7905\n",
      "val Loss: 0.4862 Acc: 0.8811\n",
      "\n",
      "Epoch: 341/500\n",
      "------------------------------\n",
      "train Loss: 0.7932 Acc: 0.7872\n",
      "val Loss: 0.7619 Acc: 0.8348\n",
      "\n",
      "Epoch: 342/500\n",
      "------------------------------\n",
      "train Loss: 0.8003 Acc: 0.7870\n",
      "val Loss: 0.8335 Acc: 0.8148\n",
      "\n",
      "Epoch: 343/500\n",
      "------------------------------\n",
      "train Loss: 0.7985 Acc: 0.7876\n",
      "val Loss: 0.6313 Acc: 0.8559\n",
      "\n",
      "Epoch: 344/500\n",
      "------------------------------\n",
      "train Loss: 0.7962 Acc: 0.7859\n",
      "val Loss: 0.7829 Acc: 0.8417\n",
      "\n",
      "Epoch: 345/500\n",
      "------------------------------\n",
      "train Loss: 0.7954 Acc: 0.7882\n",
      "val Loss: 0.7892 Acc: 0.8285\n",
      "\n",
      "Epoch: 346/500\n",
      "------------------------------\n",
      "train Loss: 0.8748 Acc: 0.7736\n",
      "val Loss: 0.4441 Acc: 0.8858\n",
      "\n",
      "Epoch: 347/500\n",
      "------------------------------\n",
      "train Loss: 0.8600 Acc: 0.7790\n",
      "val Loss: 0.9102 Acc: 0.8096\n",
      "\n",
      "Epoch: 348/500\n",
      "------------------------------\n",
      "train Loss: 0.9210 Acc: 0.7708\n",
      "val Loss: 0.5183 Acc: 0.8727\n",
      "\n",
      "Epoch: 349/500\n",
      "------------------------------\n",
      "train Loss: 0.8320 Acc: 0.7807\n",
      "val Loss: 0.5189 Acc: 0.8716\n",
      "\n",
      "Epoch: 350/500\n",
      "------------------------------\n",
      "train Loss: 0.7984 Acc: 0.7896\n",
      "val Loss: 0.6062 Acc: 0.8506\n",
      "\n",
      "Epoch: 351/500\n",
      "------------------------------\n",
      "train Loss: 0.7929 Acc: 0.7868\n",
      "val Loss: 0.6009 Acc: 0.8490\n",
      "\n",
      "Epoch: 352/500\n",
      "------------------------------\n",
      "train Loss: 0.8324 Acc: 0.7805\n",
      "val Loss: 0.4876 Acc: 0.8690\n",
      "\n",
      "Epoch: 353/500\n",
      "------------------------------\n",
      "train Loss: 0.8357 Acc: 0.7841\n",
      "val Loss: 0.7709 Acc: 0.8385\n",
      "\n",
      "Epoch: 354/500\n",
      "------------------------------\n",
      "train Loss: 0.8158 Acc: 0.7839\n",
      "val Loss: 0.6142 Acc: 0.8490\n",
      "\n",
      "Epoch: 355/500\n",
      "------------------------------\n",
      "train Loss: 0.7952 Acc: 0.7840\n",
      "val Loss: 0.5692 Acc: 0.8527\n",
      "\n",
      "Epoch: 356/500\n",
      "------------------------------\n",
      "train Loss: 0.7789 Acc: 0.7912\n",
      "val Loss: 0.6430 Acc: 0.8332\n",
      "\n",
      "Epoch: 357/500\n",
      "------------------------------\n",
      "train Loss: 0.8108 Acc: 0.7813\n",
      "val Loss: 0.5847 Acc: 0.8638\n",
      "\n",
      "Epoch: 358/500\n",
      "------------------------------\n",
      "train Loss: 0.8241 Acc: 0.7780\n",
      "val Loss: 0.4576 Acc: 0.8769\n",
      "\n",
      "Epoch: 359/500\n",
      "------------------------------\n",
      "train Loss: 0.8191 Acc: 0.7821\n",
      "val Loss: 0.4404 Acc: 0.8727\n",
      "\n",
      "Epoch: 360/500\n",
      "------------------------------\n",
      "train Loss: 0.8019 Acc: 0.7848\n",
      "val Loss: 0.5746 Acc: 0.8643\n",
      "\n",
      "Epoch: 361/500\n",
      "------------------------------\n",
      "train Loss: 0.8657 Acc: 0.7767\n",
      "val Loss: 0.6906 Acc: 0.8506\n",
      "\n",
      "Epoch: 362/500\n",
      "------------------------------\n",
      "train Loss: 0.8139 Acc: 0.7820\n",
      "val Loss: 0.6993 Acc: 0.8396\n",
      "\n",
      "Epoch: 363/500\n",
      "------------------------------\n",
      "train Loss: 0.8307 Acc: 0.7802\n",
      "val Loss: 0.7389 Acc: 0.8396\n",
      "\n",
      "Epoch: 364/500\n",
      "------------------------------\n",
      "train Loss: 0.8075 Acc: 0.7844\n",
      "val Loss: 0.5755 Acc: 0.8532\n",
      "\n",
      "Epoch: 365/500\n",
      "------------------------------\n",
      "train Loss: 0.8029 Acc: 0.7868\n",
      "val Loss: 0.7820 Acc: 0.8243\n",
      "\n",
      "Epoch: 366/500\n",
      "------------------------------\n",
      "train Loss: 0.7857 Acc: 0.7861\n",
      "val Loss: 0.5120 Acc: 0.8627\n",
      "\n",
      "Epoch: 367/500\n",
      "------------------------------\n",
      "train Loss: 0.8155 Acc: 0.7891\n",
      "val Loss: 0.6063 Acc: 0.8569\n",
      "\n",
      "Epoch: 368/500\n",
      "------------------------------\n",
      "train Loss: 0.8378 Acc: 0.7804\n",
      "val Loss: 0.6828 Acc: 0.8438\n",
      "\n",
      "Epoch: 369/500\n",
      "------------------------------\n",
      "train Loss: 0.8251 Acc: 0.7824\n",
      "val Loss: 0.7463 Acc: 0.8390\n",
      "\n",
      "Epoch: 370/500\n",
      "------------------------------\n",
      "train Loss: 0.7929 Acc: 0.7850\n",
      "val Loss: 0.6825 Acc: 0.8511\n",
      "\n",
      "Epoch: 371/500\n",
      "------------------------------\n",
      "train Loss: 0.7997 Acc: 0.7892\n",
      "val Loss: 0.4144 Acc: 0.8848\n",
      "\n",
      "Epoch: 372/500\n",
      "------------------------------\n",
      "train Loss: 0.8638 Acc: 0.7757\n",
      "val Loss: 0.8607 Acc: 0.8259\n",
      "\n",
      "Epoch: 373/500\n",
      "------------------------------\n",
      "train Loss: 0.8139 Acc: 0.7879\n",
      "val Loss: 0.7475 Acc: 0.8443\n",
      "\n",
      "Epoch: 374/500\n",
      "------------------------------\n",
      "train Loss: 0.8715 Acc: 0.7790\n",
      "val Loss: 0.5657 Acc: 0.8680\n",
      "\n",
      "Epoch: 375/500\n",
      "------------------------------\n",
      "train Loss: 0.8899 Acc: 0.7759\n",
      "val Loss: 0.4802 Acc: 0.8722\n",
      "\n",
      "Epoch: 376/500\n",
      "------------------------------\n",
      "train Loss: 0.8096 Acc: 0.7858\n",
      "val Loss: 0.8269 Acc: 0.8322\n",
      "\n",
      "Epoch: 377/500\n",
      "------------------------------\n",
      "train Loss: 0.7998 Acc: 0.7877\n",
      "val Loss: 0.8491 Acc: 0.8227\n",
      "\n",
      "Epoch: 378/500\n",
      "------------------------------\n",
      "train Loss: 0.8392 Acc: 0.7803\n",
      "val Loss: 0.9726 Acc: 0.8127\n",
      "\n",
      "Epoch: 379/500\n",
      "------------------------------\n",
      "train Loss: 0.8211 Acc: 0.7809\n",
      "val Loss: 0.5680 Acc: 0.8574\n",
      "\n",
      "Epoch: 380/500\n",
      "------------------------------\n",
      "train Loss: 0.7982 Acc: 0.7888\n",
      "val Loss: 0.7259 Acc: 0.8369\n",
      "\n",
      "Epoch: 381/500\n",
      "------------------------------\n",
      "train Loss: 0.7689 Acc: 0.7928\n",
      "val Loss: 0.4624 Acc: 0.8769\n",
      "\n",
      "Epoch: 382/500\n",
      "------------------------------\n",
      "train Loss: 0.8121 Acc: 0.7806\n",
      "val Loss: 0.8936 Acc: 0.8069\n",
      "\n",
      "Epoch: 383/500\n",
      "------------------------------\n",
      "train Loss: 0.8693 Acc: 0.7774\n",
      "val Loss: 0.7868 Acc: 0.8385\n",
      "\n",
      "Epoch: 384/500\n",
      "------------------------------\n",
      "train Loss: 0.8426 Acc: 0.7824\n",
      "val Loss: 0.8009 Acc: 0.8327\n",
      "\n",
      "Epoch: 385/500\n",
      "------------------------------\n",
      "train Loss: 0.8223 Acc: 0.7851\n",
      "val Loss: 0.5656 Acc: 0.8685\n",
      "\n",
      "Epoch: 386/500\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.8112 Acc: 0.7862\n",
      "val Loss: 0.7283 Acc: 0.8364\n",
      "\n",
      "Epoch: 387/500\n",
      "------------------------------\n",
      "train Loss: 0.8050 Acc: 0.7839\n",
      "val Loss: 0.7694 Acc: 0.8364\n",
      "\n",
      "Epoch: 388/500\n",
      "------------------------------\n",
      "train Loss: 0.8868 Acc: 0.7742\n",
      "val Loss: 0.6566 Acc: 0.8422\n",
      "\n",
      "Epoch: 389/500\n",
      "------------------------------\n",
      "train Loss: 0.8335 Acc: 0.7809\n",
      "val Loss: 0.6812 Acc: 0.8432\n",
      "\n",
      "Epoch: 390/500\n",
      "------------------------------\n",
      "train Loss: 0.7916 Acc: 0.7872\n",
      "val Loss: 0.6441 Acc: 0.8553\n",
      "\n",
      "Epoch: 391/500\n",
      "------------------------------\n",
      "train Loss: 0.8144 Acc: 0.7854\n",
      "val Loss: 0.6260 Acc: 0.8517\n",
      "\n",
      "Epoch: 392/500\n",
      "------------------------------\n",
      "train Loss: 0.8409 Acc: 0.7787\n",
      "val Loss: 0.8419 Acc: 0.8254\n",
      "\n",
      "Epoch: 393/500\n",
      "------------------------------\n",
      "train Loss: 0.8121 Acc: 0.7841\n",
      "val Loss: 0.4001 Acc: 0.8874\n",
      "\n",
      "Epoch: 394/500\n",
      "------------------------------\n",
      "train Loss: 0.7757 Acc: 0.7885\n",
      "val Loss: 0.6893 Acc: 0.8453\n",
      "\n",
      "Epoch: 395/500\n",
      "------------------------------\n",
      "train Loss: 0.8026 Acc: 0.7860\n",
      "val Loss: 0.6757 Acc: 0.8406\n",
      "\n",
      "Epoch: 396/500\n",
      "------------------------------\n",
      "train Loss: 0.8148 Acc: 0.7810\n",
      "val Loss: 0.6103 Acc: 0.8469\n",
      "\n",
      "Epoch: 397/500\n",
      "------------------------------\n",
      "train Loss: 0.8260 Acc: 0.7842\n",
      "val Loss: 0.6034 Acc: 0.8490\n",
      "\n",
      "Epoch: 398/500\n",
      "------------------------------\n",
      "train Loss: 0.8339 Acc: 0.7846\n",
      "val Loss: 0.6742 Acc: 0.8480\n",
      "\n",
      "Epoch: 399/500\n",
      "------------------------------\n",
      "train Loss: 0.8338 Acc: 0.7820\n",
      "val Loss: 0.5178 Acc: 0.8643\n",
      "\n",
      "Epoch: 400/500\n",
      "------------------------------\n",
      "train Loss: 0.7952 Acc: 0.7856\n",
      "val Loss: 0.8606 Acc: 0.8080\n",
      "\n",
      "Epoch: 401/500\n",
      "------------------------------\n",
      "train Loss: 0.8009 Acc: 0.7847\n",
      "val Loss: 0.5427 Acc: 0.8595\n",
      "\n",
      "Epoch: 402/500\n",
      "------------------------------\n",
      "train Loss: 0.7966 Acc: 0.7874\n",
      "val Loss: 0.5685 Acc: 0.8585\n",
      "\n",
      "Epoch: 403/500\n",
      "------------------------------\n",
      "train Loss: 0.8039 Acc: 0.7830\n",
      "val Loss: 0.7171 Acc: 0.8438\n",
      "\n",
      "Epoch: 404/500\n",
      "------------------------------\n",
      "train Loss: 0.8029 Acc: 0.7831\n",
      "val Loss: 0.7895 Acc: 0.8259\n",
      "\n",
      "Epoch: 405/500\n",
      "------------------------------\n",
      "train Loss: 0.8646 Acc: 0.7819\n",
      "val Loss: 0.6009 Acc: 0.8701\n",
      "\n",
      "Epoch: 406/500\n",
      "------------------------------\n",
      "train Loss: 0.8324 Acc: 0.7847\n",
      "val Loss: 0.5161 Acc: 0.8617\n",
      "\n",
      "Epoch: 407/500\n",
      "------------------------------\n",
      "train Loss: 0.8145 Acc: 0.7815\n",
      "val Loss: 0.9350 Acc: 0.8085\n",
      "\n",
      "Epoch: 408/500\n",
      "------------------------------\n",
      "train Loss: 0.8465 Acc: 0.7826\n",
      "val Loss: 0.6922 Acc: 0.8464\n",
      "\n",
      "Epoch: 409/500\n",
      "------------------------------\n",
      "train Loss: 0.8279 Acc: 0.7813\n",
      "val Loss: 0.6404 Acc: 0.8543\n",
      "\n",
      "Epoch: 410/500\n",
      "------------------------------\n",
      "train Loss: 0.8365 Acc: 0.7818\n",
      "val Loss: 0.7759 Acc: 0.8390\n",
      "\n",
      "Epoch: 411/500\n",
      "------------------------------\n",
      "train Loss: 0.8049 Acc: 0.7883\n",
      "val Loss: 0.6582 Acc: 0.8422\n",
      "\n",
      "Epoch: 412/500\n",
      "------------------------------\n",
      "train Loss: 0.8711 Acc: 0.7809\n",
      "val Loss: 0.6833 Acc: 0.8322\n",
      "\n",
      "Epoch: 413/500\n",
      "------------------------------\n",
      "train Loss: 0.8178 Acc: 0.7846\n",
      "val Loss: 0.5732 Acc: 0.8580\n",
      "\n",
      "Epoch: 414/500\n",
      "------------------------------\n",
      "train Loss: 0.8221 Acc: 0.7835\n",
      "val Loss: 0.8728 Acc: 0.8254\n",
      "\n",
      "Epoch: 415/500\n",
      "------------------------------\n",
      "train Loss: 0.8546 Acc: 0.7779\n",
      "val Loss: 0.6098 Acc: 0.8490\n",
      "\n",
      "Epoch: 416/500\n",
      "------------------------------\n",
      "train Loss: 0.8636 Acc: 0.7773\n",
      "val Loss: 0.5048 Acc: 0.8816\n",
      "\n",
      "Epoch: 417/500\n",
      "------------------------------\n",
      "train Loss: 0.8105 Acc: 0.7856\n",
      "val Loss: 0.7075 Acc: 0.8411\n",
      "\n",
      "Epoch: 418/500\n",
      "------------------------------\n",
      "train Loss: 0.8039 Acc: 0.7865\n",
      "val Loss: 0.6773 Acc: 0.8453\n",
      "\n",
      "Epoch: 419/500\n",
      "------------------------------\n",
      "train Loss: 0.8409 Acc: 0.7847\n",
      "val Loss: 0.6149 Acc: 0.8606\n",
      "\n",
      "Epoch: 420/500\n",
      "------------------------------\n",
      "train Loss: 0.8311 Acc: 0.7839\n",
      "val Loss: 1.0787 Acc: 0.8096\n",
      "\n",
      "Epoch: 421/500\n",
      "------------------------------\n",
      "train Loss: 0.8228 Acc: 0.7829\n",
      "val Loss: 0.9180 Acc: 0.8238\n",
      "\n",
      "Epoch: 422/500\n",
      "------------------------------\n",
      "train Loss: 0.8770 Acc: 0.7784\n",
      "val Loss: 0.7085 Acc: 0.8401\n",
      "\n",
      "Epoch: 423/500\n",
      "------------------------------\n",
      "train Loss: 0.8849 Acc: 0.7779\n",
      "val Loss: 0.9602 Acc: 0.7901\n",
      "\n",
      "Epoch: 424/500\n",
      "------------------------------\n",
      "train Loss: 0.8328 Acc: 0.7839\n",
      "val Loss: 0.6499 Acc: 0.8411\n",
      "\n",
      "Epoch: 425/500\n",
      "------------------------------\n",
      "train Loss: 0.8070 Acc: 0.7843\n",
      "val Loss: 0.5662 Acc: 0.8685\n",
      "\n",
      "Epoch: 426/500\n",
      "------------------------------\n",
      "train Loss: 0.8538 Acc: 0.7796\n",
      "val Loss: 0.5558 Acc: 0.8664\n",
      "\n",
      "Epoch: 427/500\n",
      "------------------------------\n",
      "train Loss: 0.8130 Acc: 0.7856\n",
      "val Loss: 0.6258 Acc: 0.8464\n",
      "\n",
      "Epoch: 428/500\n",
      "------------------------------\n",
      "train Loss: 0.8083 Acc: 0.7883\n",
      "val Loss: 0.8476 Acc: 0.8075\n",
      "\n",
      "Epoch: 429/500\n",
      "------------------------------\n",
      "train Loss: 0.8489 Acc: 0.7778\n",
      "val Loss: 0.5258 Acc: 0.8669\n",
      "\n",
      "Epoch: 430/500\n",
      "------------------------------\n",
      "train Loss: 0.8020 Acc: 0.7875\n",
      "val Loss: 0.6683 Acc: 0.8380\n",
      "\n",
      "Epoch: 431/500\n",
      "------------------------------\n",
      "train Loss: 0.8359 Acc: 0.7845\n",
      "val Loss: 0.7340 Acc: 0.8343\n",
      "\n",
      "Epoch: 432/500\n",
      "------------------------------\n",
      "train Loss: 0.8609 Acc: 0.7774\n",
      "val Loss: 0.6615 Acc: 0.8517\n",
      "\n",
      "Epoch: 433/500\n",
      "------------------------------\n",
      "train Loss: 0.7846 Acc: 0.7927\n",
      "val Loss: 0.5379 Acc: 0.8553\n",
      "\n",
      "Epoch: 434/500\n",
      "------------------------------\n",
      "train Loss: 0.8361 Acc: 0.7807\n",
      "val Loss: 0.6690 Acc: 0.8427\n",
      "\n",
      "Epoch: 435/500\n",
      "------------------------------\n",
      "train Loss: 0.8245 Acc: 0.7816\n",
      "val Loss: 0.4778 Acc: 0.8738\n",
      "\n",
      "Epoch: 436/500\n",
      "------------------------------\n",
      "train Loss: 0.8623 Acc: 0.7787\n",
      "val Loss: 0.5930 Acc: 0.8506\n",
      "\n",
      "Epoch: 437/500\n",
      "------------------------------\n",
      "train Loss: 0.8447 Acc: 0.7826\n",
      "val Loss: 0.8818 Acc: 0.8217\n",
      "\n",
      "Epoch: 438/500\n",
      "------------------------------\n",
      "train Loss: 0.8145 Acc: 0.7931\n",
      "val Loss: 0.7822 Acc: 0.8359\n",
      "\n",
      "Epoch: 439/500\n",
      "------------------------------\n",
      "train Loss: 0.8022 Acc: 0.7876\n",
      "val Loss: 0.5856 Acc: 0.8595\n",
      "\n",
      "Epoch: 440/500\n",
      "------------------------------\n",
      "train Loss: 0.8320 Acc: 0.7837\n",
      "val Loss: 0.5890 Acc: 0.8606\n",
      "\n",
      "Epoch: 441/500\n",
      "------------------------------\n",
      "train Loss: 0.8144 Acc: 0.7818\n",
      "val Loss: 0.7513 Acc: 0.8311\n",
      "\n",
      "Epoch: 442/500\n",
      "------------------------------\n",
      "train Loss: 0.8635 Acc: 0.7807\n",
      "val Loss: 0.8984 Acc: 0.8269\n",
      "\n",
      "Epoch: 443/500\n",
      "------------------------------\n",
      "train Loss: 0.8188 Acc: 0.7822\n",
      "val Loss: 0.6373 Acc: 0.8438\n",
      "\n",
      "Epoch: 444/500\n",
      "------------------------------\n",
      "train Loss: 0.7974 Acc: 0.7895\n",
      "val Loss: 0.5400 Acc: 0.8695\n",
      "\n",
      "Epoch: 445/500\n",
      "------------------------------\n",
      "train Loss: 0.7982 Acc: 0.7857\n",
      "val Loss: 0.6669 Acc: 0.8538\n",
      "\n",
      "Epoch: 446/500\n",
      "------------------------------\n",
      "train Loss: 0.8472 Acc: 0.7771\n",
      "val Loss: 0.8297 Acc: 0.8101\n",
      "\n",
      "Epoch: 447/500\n",
      "------------------------------\n",
      "train Loss: 0.8083 Acc: 0.7860\n",
      "val Loss: 0.5134 Acc: 0.8664\n",
      "\n",
      "Epoch: 448/500\n",
      "------------------------------\n",
      "train Loss: 0.8129 Acc: 0.7847\n",
      "val Loss: 0.6054 Acc: 0.8490\n",
      "\n",
      "Epoch: 449/500\n",
      "------------------------------\n",
      "train Loss: 0.8105 Acc: 0.7875\n",
      "val Loss: 0.6841 Acc: 0.8375\n",
      "\n",
      "Epoch: 450/500\n",
      "------------------------------\n",
      "train Loss: 0.8439 Acc: 0.7816\n",
      "val Loss: 0.6496 Acc: 0.8538\n",
      "\n",
      "Epoch: 451/500\n",
      "------------------------------\n",
      "train Loss: 0.8696 Acc: 0.7798\n",
      "val Loss: 0.6875 Acc: 0.8480\n",
      "\n",
      "Epoch: 452/500\n",
      "------------------------------\n",
      "train Loss: 0.8313 Acc: 0.7822\n",
      "val Loss: 0.6436 Acc: 0.8548\n",
      "\n",
      "Epoch: 453/500\n",
      "------------------------------\n",
      "train Loss: 0.8470 Acc: 0.7778\n",
      "val Loss: 0.7515 Acc: 0.8369\n",
      "\n",
      "Epoch: 454/500\n",
      "------------------------------\n",
      "train Loss: 0.8077 Acc: 0.7833\n",
      "val Loss: 0.8361 Acc: 0.8259\n",
      "\n",
      "Epoch: 455/500\n",
      "------------------------------\n",
      "train Loss: 0.7967 Acc: 0.7903\n",
      "val Loss: 0.7086 Acc: 0.8438\n",
      "\n",
      "Epoch: 456/500\n",
      "------------------------------\n",
      "train Loss: 0.8594 Acc: 0.7792\n",
      "val Loss: 0.7987 Acc: 0.8275\n",
      "\n",
      "Epoch: 457/500\n",
      "------------------------------\n",
      "train Loss: 0.8459 Acc: 0.7826\n",
      "val Loss: 0.6328 Acc: 0.8506\n",
      "\n",
      "Epoch: 458/500\n",
      "------------------------------\n",
      "train Loss: 0.8151 Acc: 0.7868\n",
      "val Loss: 0.6345 Acc: 0.8490\n",
      "\n",
      "Epoch: 459/500\n",
      "------------------------------\n",
      "train Loss: 0.8431 Acc: 0.7825\n",
      "val Loss: 0.7873 Acc: 0.8238\n",
      "\n",
      "Epoch: 460/500\n",
      "------------------------------\n",
      "train Loss: 0.8427 Acc: 0.7838\n",
      "val Loss: 0.8222 Acc: 0.8259\n",
      "\n",
      "Epoch: 461/500\n",
      "------------------------------\n",
      "train Loss: 0.8687 Acc: 0.7776\n",
      "val Loss: 0.5746 Acc: 0.8543\n",
      "\n",
      "Epoch: 462/500\n",
      "------------------------------\n",
      "train Loss: 0.8141 Acc: 0.7848\n",
      "val Loss: 0.6554 Acc: 0.8490\n",
      "\n",
      "Epoch: 463/500\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.8008 Acc: 0.7893\n",
      "val Loss: 0.5794 Acc: 0.8522\n",
      "\n",
      "Epoch: 464/500\n",
      "------------------------------\n",
      "train Loss: 0.8075 Acc: 0.7876\n",
      "val Loss: 0.6974 Acc: 0.8264\n",
      "\n",
      "Epoch: 465/500\n",
      "------------------------------\n",
      "train Loss: 0.8098 Acc: 0.7861\n",
      "val Loss: 0.7025 Acc: 0.8411\n",
      "\n",
      "Epoch: 466/500\n",
      "------------------------------\n",
      "train Loss: 0.8181 Acc: 0.7887\n",
      "val Loss: 0.6112 Acc: 0.8585\n",
      "\n",
      "Epoch: 467/500\n",
      "------------------------------\n",
      "train Loss: 0.7829 Acc: 0.7886\n",
      "val Loss: 0.4885 Acc: 0.8738\n",
      "\n",
      "Epoch: 468/500\n",
      "------------------------------\n",
      "train Loss: 0.7827 Acc: 0.7950\n",
      "val Loss: 0.5423 Acc: 0.8632\n",
      "\n",
      "Epoch: 469/500\n",
      "------------------------------\n",
      "train Loss: 0.8109 Acc: 0.7874\n",
      "val Loss: 0.6833 Acc: 0.8417\n",
      "\n",
      "Epoch: 470/500\n",
      "------------------------------\n",
      "train Loss: 0.8205 Acc: 0.7854\n",
      "val Loss: 0.9438 Acc: 0.8238\n",
      "\n",
      "Epoch: 471/500\n",
      "------------------------------\n",
      "train Loss: 0.8272 Acc: 0.7855\n",
      "val Loss: 0.8531 Acc: 0.8254\n",
      "\n",
      "Epoch: 472/500\n",
      "------------------------------\n",
      "train Loss: 0.8219 Acc: 0.7854\n",
      "val Loss: 0.6191 Acc: 0.8595\n",
      "\n",
      "Epoch: 473/500\n",
      "------------------------------\n",
      "train Loss: 0.8263 Acc: 0.7889\n",
      "val Loss: 0.5776 Acc: 0.8659\n",
      "\n",
      "Epoch: 474/500\n",
      "------------------------------\n",
      "train Loss: 0.8136 Acc: 0.7884\n",
      "val Loss: 0.8741 Acc: 0.8275\n",
      "\n",
      "Epoch: 475/500\n",
      "------------------------------\n",
      "train Loss: 0.7780 Acc: 0.7886\n",
      "val Loss: 0.4835 Acc: 0.8711\n",
      "\n",
      "Epoch: 476/500\n",
      "------------------------------\n",
      "train Loss: 0.8254 Acc: 0.7822\n",
      "val Loss: 0.5987 Acc: 0.8590\n",
      "\n",
      "Epoch: 477/500\n",
      "------------------------------\n",
      "train Loss: 0.7756 Acc: 0.7897\n",
      "val Loss: 0.7058 Acc: 0.8401\n",
      "\n",
      "Epoch: 478/500\n",
      "------------------------------\n",
      "train Loss: 0.7830 Acc: 0.7860\n",
      "val Loss: 0.5989 Acc: 0.8580\n",
      "\n",
      "Epoch: 479/500\n",
      "------------------------------\n",
      "train Loss: 0.7963 Acc: 0.7862\n",
      "val Loss: 0.4950 Acc: 0.8790\n",
      "\n",
      "Epoch: 480/500\n",
      "------------------------------\n",
      "train Loss: 0.7885 Acc: 0.7848\n",
      "val Loss: 0.5169 Acc: 0.8680\n",
      "\n",
      "Epoch: 481/500\n",
      "------------------------------\n",
      "train Loss: 0.8006 Acc: 0.7869\n",
      "val Loss: 0.7274 Acc: 0.8390\n",
      "\n",
      "Epoch: 482/500\n",
      "------------------------------\n",
      "train Loss: 0.8033 Acc: 0.7885\n",
      "val Loss: 0.6350 Acc: 0.8538\n",
      "\n",
      "Epoch: 483/500\n",
      "------------------------------\n",
      "train Loss: 0.8557 Acc: 0.7809\n",
      "val Loss: 0.6347 Acc: 0.8438\n",
      "\n",
      "Epoch: 484/500\n",
      "------------------------------\n",
      "train Loss: 0.8424 Acc: 0.7800\n",
      "val Loss: 0.8046 Acc: 0.8401\n",
      "\n",
      "Epoch: 485/500\n",
      "------------------------------\n",
      "train Loss: 0.7897 Acc: 0.7925\n",
      "val Loss: 1.2117 Acc: 0.8064\n",
      "\n",
      "Epoch: 486/500\n",
      "------------------------------\n",
      "train Loss: 0.8058 Acc: 0.7883\n",
      "val Loss: 0.5476 Acc: 0.8680\n",
      "\n",
      "Epoch: 487/500\n",
      "------------------------------\n",
      "train Loss: 0.8393 Acc: 0.7825\n",
      "val Loss: 0.5835 Acc: 0.8701\n",
      "\n",
      "Epoch: 488/500\n",
      "------------------------------\n",
      "train Loss: 0.7712 Acc: 0.7877\n",
      "val Loss: 0.6779 Acc: 0.8448\n",
      "\n",
      "Epoch: 489/500\n",
      "------------------------------\n",
      "train Loss: 0.8028 Acc: 0.7872\n",
      "val Loss: 0.6710 Acc: 0.8474\n",
      "\n",
      "Epoch: 490/500\n",
      "------------------------------\n",
      "train Loss: 0.8264 Acc: 0.7813\n",
      "val Loss: 0.5632 Acc: 0.8559\n",
      "\n",
      "Epoch: 491/500\n",
      "------------------------------\n",
      "train Loss: 0.7985 Acc: 0.7878\n",
      "val Loss: 0.6786 Acc: 0.8417\n",
      "\n",
      "Epoch: 492/500\n",
      "------------------------------\n",
      "train Loss: 0.8590 Acc: 0.7804\n",
      "val Loss: 0.5564 Acc: 0.8659\n",
      "\n",
      "Epoch: 493/500\n",
      "------------------------------\n",
      "train Loss: 0.7954 Acc: 0.7870\n",
      "val Loss: 0.7695 Acc: 0.8217\n",
      "\n",
      "Epoch: 494/500\n",
      "------------------------------\n",
      "train Loss: 0.8135 Acc: 0.7843\n",
      "val Loss: 0.7617 Acc: 0.8385\n",
      "\n",
      "Epoch: 495/500\n",
      "------------------------------\n",
      "train Loss: 0.8360 Acc: 0.7815\n",
      "val Loss: 0.6400 Acc: 0.8569\n",
      "\n",
      "Epoch: 496/500\n",
      "------------------------------\n",
      "train Loss: 0.7817 Acc: 0.7870\n",
      "val Loss: 0.6521 Acc: 0.8380\n",
      "\n",
      "Epoch: 497/500\n",
      "------------------------------\n",
      "train Loss: 0.8022 Acc: 0.7851\n",
      "val Loss: 0.8097 Acc: 0.8264\n",
      "\n",
      "Epoch: 498/500\n",
      "------------------------------\n",
      "train Loss: 0.8586 Acc: 0.7813\n",
      "val Loss: 1.0155 Acc: 0.8101\n",
      "\n",
      "Epoch: 499/500\n",
      "------------------------------\n",
      "train Loss: 0.8077 Acc: 0.7894\n",
      "val Loss: 0.8050 Acc: 0.8248\n",
      "\n",
      "Epoch: 500/500\n",
      "------------------------------\n",
      "train Loss: 0.8492 Acc: 0.7791\n",
      "val Loss: 0.6291 Acc: 0.8511\n",
      "\n",
      "-----------------------------------\n",
      "Training completed in 680m 20s\n",
      "Best validation accuracy: 0.888480\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=500):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    train_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch: {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 30)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            else: \n",
    "                train_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('-' * 35)\n",
    "    print('Training completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best validation accuracy: {:4f}'.format(best_acc))\n",
    "    print('-' * 35)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, train_acc_history\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model_ft, val_hist, train_hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Getting the learning process history in graph\n",
    "\n",
    "plt.title(\"Accuracy vs Number of Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),train_hist,label=\"Training\")\n",
    "plt.plot(range(1,num_epochs+1),val_hist,label=\"Validation\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "\n",
    "model_path = '../Model/Pesticides_Prescription_Model.sav'\n",
    "torch.save(model_ft.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the class labels\n",
    "\n",
    "class_labels_path = '../Model/Class_Labels_Pesticides_Prescription_Model.csv'\n",
    "\n",
    "def save_class_labels(class_labels_path):\n",
    "    labels_dict = image_datasets['train'].class_to_idx\n",
    "    labels_df = pd.DataFrame.from_dict(data=labels_dict, orient='index', columns=['idx'])\n",
    "    labels_df['Label'] = labels_df.index\n",
    "    labels_df.reset_index(drop=True, inplace=True)\n",
    "    labels_df.to_csv(class_labels_path, header=None, index=False)\n",
    "\n",
    "save_class_labels(class_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
